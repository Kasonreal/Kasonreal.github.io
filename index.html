<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="Kason&apos;s technology blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Kason&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Kason&#39;s Blog">
<meta property="og:description" content="Kason&apos;s technology blog">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kason&#39;s Blog">
<meta name="twitter:description" content="Kason&apos;s technology blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>Kason's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Kason's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/13/用户画像推荐系统/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kason">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/westbrook.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kason's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/13/用户画像推荐系统/" itemprop="url">用户画像推荐系统</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-13T21:15:33+08:00">
                2020-01-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/" itemprop="url" rel="index">
                    <span itemprop="name">推荐系统</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/01/13/用户画像推荐系统/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2020/01/13/用户画像推荐系统/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="用户画像推荐系统"><a href="#用户画像推荐系统" class="headerlink" title="用户画像推荐系统"></a>用户画像推荐系统</h1><p>用户画像是特征空间中的<strong>高维向量</strong></p>
<p>每个标签是特征空间中的<strong>基向量</strong></p>
<h2 id="用户画像系统流程"><a href="#用户画像系统流程" class="headerlink" title="用户画像系统流程"></a>用户画像系统流程</h2><p><img src="http://ww1.sinaimg.cn/large/006M8Ovlgy1gavasel2wrj31680o3gv5.jpg" alt="用户画像系统流程.png"></p>
<h3 id="一、明确问题"><a href="#一、明确问题" class="headerlink" title="一、明确问题"></a>一、明确问题</h3><ul>
<li>我们的需求和数据的匹配</li>
<li>我们是用来做分类、聚类、推荐还是回归等</li>
<li>数据的规模、重要特征的覆盖度等</li>
</ul>
<h3 id="二、-数据预处理"><a href="#二、-数据预处理" class="headerlink" title="二、 数据预处理"></a>二、 数据预处理</h3><ul>
<li>数据集成、冗余处理、冲突处理、采样、清洗、缺失值处理、噪声处理</li>
</ul>
<h3 id="三、-特征工程"><a href="#三、-特征工程" class="headerlink" title="三、 特征工程"></a>三、 特征工程</h3><p>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。</p>
<p><strong>特征提取</strong></p>
<ul>
<li>业务日志</li>
<li>WEB公开数据抓取</li>
<li>第三方合作</li>
</ul>
<p><strong>特征处理</strong></p>
<ul>
<li>特征清洗</li>
<li>特征预处理： 特征选择、特征组合、降维等</li>
</ul>
<p><strong>特征监控</strong></p>
<ul>
<li>指标：时效性、覆盖率和异常值</li>
<li>可视化&amp;预警</li>
</ul>
<h3 id="四、-模型和算法"><a href="#四、-模型和算法" class="headerlink" title="四、 模型和算法"></a>四、 模型和算法</h3><p><img src="http://ww1.sinaimg.cn/large/006M8Ovlgy1gavb3fyt1xj315q0opao7.jpg" alt="推荐模型和算法.png"></p>

          
        
      
    </div>
    
    
    

    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/11/Flink商品实时推荐系统/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kason">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/westbrook.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kason's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/11/Flink商品实时推荐系统/" itemprop="url">Flink商品实时推荐系统</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-11T14:55:39+08:00">
                2020-01-11
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/" itemprop="url" rel="index">
                    <span itemprop="name">推荐系统</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/01/11/Flink商品实时推荐系统/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2020/01/11/Flink商品实时推荐系统/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Flink商品实时推荐系统"><a href="#Flink商品实时推荐系统" class="headerlink" title="Flink商品实时推荐系统"></a>Flink商品实时推荐系统</h1>
          
        
      
    </div>
    
    
    

    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/26/What Are You Known For Learning User Topical Profiles with Implicit and Explicit Footprints/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kason">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/westbrook.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kason's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/12/26/What Are You Known For Learning User Topical Profiles with Implicit and Explicit Footprints/" itemprop="url">What Are You Known For? Learning User Topical Profiles with Implicit and Explicit Footprints</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-12-26T14:44:41+08:00">
                2019-12-26
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/论文合集/" itemprop="url" rel="index">
                    <span itemprop="name">论文合集</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/12/26/What Are You Known For Learning User Topical Profiles with Implicit and Explicit Footprints/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/12/26/What Are You Known For Learning User Topical Profiles with Implicit and Explicit Footprints/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h1><p> 你以什么著称？通过隐式和显示足迹学习用户主题画像</p>
<h2 id="Research-Objective"><a href="#Research-Objective" class="headerlink" title="Research Objective"></a>Research Objective</h2><p>通过多个足迹（多源异构）构造用户主题画像</p>
<h2 id="Background-and-Problems"><a href="#Background-and-Problems" class="headerlink" title="Background and Problems"></a>Background and Problems</h2><ol>
<li>为什么</li>
<li><p>挑战</p>
</li>
<li><p>先前研究</p>
</li>
</ol>
<h2 id="Method-s"><a href="#Method-s" class="headerlink" title="Method(s)"></a>Method(s)</h2><ul>
<li>解决了匿名化Web浏览历史记录的问题，同时保留服务实用性</li>
<li>提出PBooster框架，用于量化用户隐私和在线服务质量之间的权衡，并进行验证</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>作者给了哪些结论，哪些是strong conclusions,哪些是weak的conclusions？哪些构思？</p>
<p>同时考虑隐私性和效用性，设置 $\lambda$=10时，可以返回尽可能高的隐私性，同时保持与原始数据相当的效用。</p>
<h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>作者如何评估自己的方法，实验的setup是什么样的，有没有问题或者可以借鉴的地方</p>
<p>数据来源+重要指标+模型步骤+每个步骤得出的结论</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>写完笔记之后最后填，概述文章的内容。切记需要通过自己的思考，用自己的语言描述</p>
<h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><p>额外笔记</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>(optional)列出相关性高的文献，一遍之后可以继续track</p>

          
        
      
    </div>
    
    
    

    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/22/机器学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kason">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/westbrook.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kason's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/22/机器学习/" itemprop="url">机器学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-22T10:41:03+08:00">
                2019-11-22
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/11/22/机器学习/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/11/22/机器学习/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="机器学习引言"><a href="#机器学习引言" class="headerlink" title="机器学习引言"></a>机器学习引言</h1><h2 id="监督学习-Supervised-learning"><a href="#监督学习-Supervised-learning" class="headerlink" title="监督学习(Supervised learning)"></a>监督学习(Supervised learning)</h2><p>数据集中的每个样本都有相应的“正确答案”，根据这些样本作出<em>预测</em>。</p>
<h3 id="监督学习的类型"><a href="#监督学习的类型" class="headerlink" title="监督学习的类型"></a>监督学习的类型</h3><ul>
<li>回归： 推出一个<strong>连续</strong>的输出        </li>
<li>分类： 推出一组<strong>离散</strong>的结果</li>
</ul>
<h2 id="非监督学习-Unsupervised-learning"><a href="#非监督学习-Unsupervised-learning" class="headerlink" title="非监督学习(Unsupervised learning)"></a>非监督学习(Unsupervised learning)</h2><p>在未加标签的数据中，试图找到隐藏的结构。</p>
<h3 id="非监督学习的类型"><a href="#非监督学习的类型" class="headerlink" title="非监督学习的类型"></a>非监督学习的类型</h3><p><em>聚类</em>，<em>降维</em>，<em>隐马尔可夫模型</em>等。</p>
<h3 id="聚类的应用："><a href="#聚类的应用：" class="headerlink" title="聚类的应用："></a><strong>聚类</strong>的应用：</h3><ol>
<li>谷歌新闻：将非常多的新闻事件<em>自动地</em>聚类到一起  </li>
<li>基因学的应用：根据基因将个体聚类到不同的类或组  </li>
<li>社交网络的分析：根据Facebook或email自动给出朋友的分组  </li>
<li>市场分析  </li>
</ol>
<h1 id="线性回归-Linear-Regression"><a href="#线性回归-Linear-Regression" class="headerlink" title="线性回归(Linear Regression)"></a>线性回归(Linear Regression)</h1><h2 id="描述回归问题的标记"><a href="#描述回归问题的标记" class="headerlink" title="描述回归问题的标记"></a>描述回归问题的标记</h2><p>$m$ 代表训练集中实例的数量</p>
<p>$x$  代表特征/输入变量</p>
<p>$y$ 代表目标变量/输出变量</p>
<p>$\left( x,y \right)$ 代表训练集中的实例</p>
<p>$\left(x^\left(i\right),y^\left(i\right)\right)$ 代表第$i$ 个观察实例</p>
<p>$h$  代表学习算法的解决方案或函数也称为假设（<strong>hypothesis</strong>）</p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g7qq9p01v9j309f07jweb.jpg" alt="回归流程.png"></p>
<p><strong>h的表达形式</strong>：</p>
<p>一种表达形式 $h_\theta \left( x \right)=\theta_{0} + \theta_{1}x_1 +···+\theta_nx_n$</p>
<p><strong>代价函数：</strong>（均方误差）</p>
<p>$J(\theta_0,\theta_1,···\theta_n)=\frac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2$</p>
<p>这里除以m是希望之后计算梯度时大小不随样本数量的增多而增大</p>
<p><strong>批量梯度下降算法：</strong></p>
<p>$\theta_j:=\theta_j-\alpha\frac{\partial }{\partial \theta_j}J(\theta_0,\theta_1,···\theta_n)  $    </p>
<p>线性回归问题可以改写为：</p>
<p>$\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)}·x_0^{(i)})$               $x_0^{(i)}=1$</p>
<p>$\theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)}·x_1^{(i)})$ </p>
<p>$···$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">theta = np.zeros(X.shape[<span class="number">1</span>]) <span class="comment"># theta代表特征（变量）数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta, X, y)</span>:</span>  <span class="comment"># 求导，这里将x0赋为1，同其他变量一起求导</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]  <span class="comment"># 样本数</span></span><br><span class="line">    inner = X.T @ (X @ theta - y)</span><br><span class="line">    <span class="keyword">return</span> inner / m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_gradient_decent</span><span class="params">(theta, X, y, epoch, alpha=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">    _theta = theta.copy()</span><br><span class="line">    cost_data = [lr_cost(theta, X, y)]</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(epoch):</span><br><span class="line">        _theta = _theta - alpha * gradient(_theta, X, y)</span><br><span class="line">        cost_data.append(lr_cost(_theta, X, y))</span><br><span class="line">    <span class="keyword">return</span> _theta, cost_data</span><br></pre></td></tr></table></figure>
<h2 id="梯度下降实践"><a href="#梯度下降实践" class="headerlink" title="梯度下降实践"></a>梯度下降实践</h2><p><strong>特征缩放</strong></p>
<p>最简单的方法:   $x_n=\frac{x_n-\mu_n}{s_n}$，其中$\mu_n$是平均值，$s_n$是范围</p>
<p><strong>学习率</strong></p>
<p>画出代价函数随迭代次数的图像。若出现上升情形，很可能是因为学习率$\alpha$取值过大</p>
<p>通常考虑$\alpha=0.03,0.3,1,3,10$</p>
<h2 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h2><p>算术求解出最优$\theta$</p>
<p>$\theta=(X^TX)^{-1}X^Ty$</p>
<p>大多数情况下，$(X^TX)^{-1}$是可逆的，若不可逆，有以下<strong>两种情况</strong>：</p>
<ul>
<li>特征值里有一些多余的特征，如果其中$x_1$和$x_2$是线性相关的，会导致不可逆，我们需要删除重复特征里的其中一个</li>
<li>特征数量太多(m&lt;=n)，导致不可逆。可以用较少的特征来反映尽可能多内容，或者考虑使用正则化的方法。</li>
</ul>
<p><strong>$\theta$的推导过程（略）</strong></p>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>在分类问题中，逻辑回归（Logistic Regression)是目前最流行使用最广泛的一种学习算法。</p>
<h2 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h2><p><strong>模型假设</strong></p>
<p>$h_\theta(x)=g(\theta^TX)$  其中 X代表特征向量，g代表逻辑函数</p>
<p>一个常用的逻辑函数为S形函数，公式为$g(z)=\frac{1}{1+e^{-z}}$</p>
<p><strong>代价函数</strong>（交叉熵）</p>
<p>若仍使用平方误差，得到的代价函数是非凸函数，因此采用交叉熵</p>
<p>$J(\theta ) = -\frac{1}{m}\left[\sum_{i=1}^{m}y^{(i)}log(h_\theta (x^{(i)}))+(1-y^{(i)})log(1-h_\theta (x^{(i)})) \right]$</p>
<p>其中，$h_\theta (x^{(i)}) = \frac{1}{1+e^{-\theta^\mathrm {T} x}}$</p>
<p><strong>梯度下降</strong></p>
<p>$\frac{\partial J(\theta)}{\partial \theta_{j}} = \frac{1}{m}\left[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_{j}^{(i)}\right]$</p>
<p>$\theta_j:=\theta_j-\alpha\frac{1}{m}\left[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_{j}^{(i)}\right]$</p>
<p>即使更新参数的规则看起来和线性回归一致，由于假设的定义发生了变化，所以是完全不同的</p>
<p><strong>高级优化</strong></p>
<p>除了梯度下降法，我们还可以采用其他方法进行优化，比如共轭梯度法，BFGS（变尺度法），L-BFGS（限制变尺度法）。这三种算法不需要手动选择学习率$\alpha$，它们内部有一个<strong>线性搜索</strong>算法</p>
<h2 id="一对多"><a href="#一对多" class="headerlink" title="一对多"></a>一对多</h2><p>使用二分类的思想，将一个类别标记为正向类，其他的所有类标记为负向类，进行多次分类。</p>
<p>得到的一系列模型记为： $h_\theta^{(i)}(x)=p(y=i|x;\theta)$其中：i=(1,2,…k)</p>
<p><strong>目标</strong></p>
<p>$\max_i h_\theta^{(i)}(x)$</p>
<p>在n个分类器中输入x，选择一个让$h_\theta^{(i)}(x)$最大的i</p>
<h2 id="Exercise-多项式特征映射"><a href="#Exercise-多项式特征映射" class="headerlink" title="Exercise(多项式特征映射)"></a>Exercise(多项式特征映射)</h2><p>如果样本量多，逻辑问题很复杂<strong>原始特征</strong>只有x1，x2，可以用多项式创建更多的特征。因为更多的特征可以得到的分割线可以是<strong>高阶</strong>函数的形状</p>
<p>eg: 有a,b两个特征，那么它的2次多项式的次数为[1,a,b,$a^2$,ab,$b^2$]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_mapping</span><span class="params">(x, y, power, as_ndarray=False)</span>:</span></span><br><span class="line"><span class="comment">#     """return mapped features as ndarray or dataframe"""</span></span><br><span class="line">    <span class="comment"># data = &#123;&#125;</span></span><br><span class="line">    <span class="comment"># # inclusive</span></span><br><span class="line">    <span class="comment"># for i in np.arange(power + 1):</span></span><br><span class="line">    <span class="comment">#     for p in np.arange(i + 1):</span></span><br><span class="line">    <span class="comment">#         data["f&#123;&#125;&#123;&#125;".format(i - p, p)] = np.power(x, i - p) * np.power(y, p)</span></span><br><span class="line"></span><br><span class="line">    data = &#123;<span class="string">"f&#123;&#125;&#123;&#125;"</span>.format(i - p, p): np.power(x, i - p) * np.power(y, p)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(power + <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> np.arange(i + <span class="number">1</span>)</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure>
<p>其中power的值代表最高阶次数</p>
<h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><ol>
<li>逻辑回归为什么是线性的？而神经网络不是线性的？</li>
</ol>
<p>主要依据<strong>决策边界</strong>。LR的决策边界是线性的。$\hat\theta·x=0$是线性的</p>
<p>而神经网络的边界不是线性的。</p>
<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p><strong>如何解决过拟合：</strong></p>
<ul>
<li>减少特征变量</li>
<li>正则化。保留所有的特征，但是减少参数的大小  </li>
</ul>
<h2 id="正则化线性回归"><a href="#正则化线性回归" class="headerlink" title="正则化线性回归"></a>正则化线性回归</h2><p><strong>正则化线性回归的代价函数：</strong></p>
<p>$J( \theta)=\frac{1}{2m}[ {\sum\limits_{i = 1}^m ( h_\theta( x^{(i)}  - y^{(i)}) ^2+ \lambda \sum\limits_{j = 1}^n {\theta _j^2} } ]$</p>
<p>此时梯度下降法：</p>
<p>Repeat{</p>
<p>${\theta _0}: = {\theta _0} - \alpha \left[ {\frac{1}{m}\sum\limits_{i = 1}^m {\left( {h_\theta \left( {x^{\left( i \right)}} \right) - {y^{\left( i \right)}}} \right)x_0^{\left( i \right)}} } \right]$</p>
<script type="math/tex; mode=display">{\theta _j}: = {\theta _j} - \alpha \left[ {\frac{1}{m}\sum\limits_{i = 1}^m {\left( {h_\theta \left( {x^{\left( i \right)}} \right) - {y^{\left( i \right)}}} \right)x_j^{\left( i \right)} + \frac{\lambda }{m}{\theta _j}} } \right]\left( {j = 1,2,...,n} \right)</script><p>}</p>
<p><strong>正规方程求解：</strong></p>
<script type="math/tex; mode=display">\theta = {\left( {X^TX + \lambda \left[ {\begin{array}{*{20}{c}} 0&0&0&0\\ 0&1&0&0\\ .&.&.&.\\ 0&0&0&1 \end{array}} \right]} \right)^{ - 1}}{X^T}y</script><p>图中的矩阵尺寸为（n+1)*（n+1)</p>
<h2 id="正则化逻辑回归"><a href="#正则化逻辑回归" class="headerlink" title="正则化逻辑回归"></a>正则化逻辑回归</h2><p> <strong>正规化逻辑回归代价函数</strong></p>
<p>$J(\theta ) = -\frac{1}{m}\left[\sum_{i=1}^{m}y^{(i)}log(h_\theta (x^{(i)}))+(1-y^{(i)})log(1-h_\theta (x^{(i)})) \right]+\frac{\lambda}{2m}\sum_\limits{j=1}^n\theta_j^2$</p>
<p>此时梯度下降法：</p>
<p>Repeat{</p>
<p>${\theta _0}: = {\theta _0} - \alpha \left[ {\frac{1}{m}\sum\limits_{i = 1}^m {\left( {h_\theta \left( {x^{\left( i \right)}} \right) - {y^{\left( i \right)}}} \right)x_0^{\left( i \right)}} } \right]$</p>
<p>${\theta _j}: = {\theta _j} - \alpha \left[ {\frac{1}{m}\sum\limits_{i = 1}^m {\left( {h_\theta \left( {x^{\left( i \right)}} \right) - {y^{\left( i \right)}}} \right)x_j^{\left( i \right)} + \frac{\lambda }{m}{\theta _j}} } \right]\left( {j = 1,2,…,n} \right)$</p>
<p>}</p>
<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><p><strong>代价函数</strong></p>
<p>$J(\Theta) = -\frac{ 1 }{ m }[\sum_\limits{ i=1 }^{ m } \sum_\limits{ k=1 }^{ k } ({y_k^{(i)} \log(h_\Theta(x^{(i)}))_k + (1 - y_k^{(i)}) \log (1 - (h_\Theta(x^{(i)}))_k})]+\frac{\lambda}{2m}\sum_\limits{l=1}^{L-1}\sum_\limits{i=1}^{s_l}\sum_\limits{j=1}^{s_{l+1}}(\Theta_{ji}^{(l)})^2)$</p>
<p>其中 ${\left( {h_\Theta }\left( x \right) \right)_i}$ = 神经网络的第i个输出</p>
<p>正则化项中：最里层的j循环所有的行，i循环所有的列，最外层是神经元的层数。</p>
<h2 id="向前传播"><a href="#向前传播" class="headerlink" title="向前传播"></a>向前传播</h2><p>公式$a_j^l=\sigma(\sum\limits_kw_{jk}^l\alpha_k^{l-1}+b_j^l)$</p>
<p>$b_j^l$表示在$l^{th}$层第$j^{th}$个神经元的偏置，$\alpha_j^{l}$表示$l^{th}$层第$j^{th}$个神经元的激活值</p>
<p>$w_{jk}^l$是从$(l-1)^{th}$层的第$k^{th}$个神经元到$l^{th}$层的第$j^{th}$个神经元的连接上的权重</p>
<p>对于一个四层的神经网络</p>
<p>$\begin{array}{l} {a^{\left( 1 \right)}} = x\\ {z^{\left( 2 \right)}} = {\Theta ^{\left( 1 \right)}}{a^{\left( 1 \right)}}\\ {a^{\left( 2 \right)}} = g\left( {z^{\left( 2 \right)}} \right)\left( { + a_0^{\left( 2 \right)}} \right)\\ {z^{\left( 3 \right)}} = {\Theta ^{\left( 2 \right)}}{a^{\left( 2 \right)}}\\ {a^{\left( 3 \right)}} = g\left( {z^{\left( 3 \right)}} \right)\left( { + a_0^{\left( 3 \right)}} \right)\\ {z^{\left( 4 \right)}} = {\Theta ^{\left( 3 \right)}}{a^{\left( 3 \right)}}\\ {a^{\left( 4 \right)}} = {h_\Theta }\left( x \right) = g\left( {z^{\left( 4 \right)}} \right) \end{array}$</p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>反向传播的目标是计算代价函数C分别关于w和b的偏导数$\frac{\partial C}{\partial w}$和$\frac{\partial C}{\partial b}$</p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9ah3vzgxdj30ef05lq39.jpg" alt="反向传播1.png"></p>
<p><strong>反向传播算法描述</strong></p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9ahes3t3aj30jv05wt9q.jpg" alt="反向传播2.png"></p>
<p><strong>梯度检查</strong></p>
<p>$\frac{\text{d}}{\text{d}\Theta}J(\Theta)\approx\frac{J(\Theta+\epsilon)-J(\Theta-\epsilon)}{2\epsilon}$</p>
<p>没有使用这种方法计算偏导数的原因：反向传播可以同时计算所有的偏导数$\frac{\partial C}{\partial w}$</p>
<p><strong>随机初始化</strong></p>
<p>初始化所有的$\theta$为0是不行的，因此随机初始化参数矩阵</p>
<h2 id="Exercise（手写数字识别）"><a href="#Exercise（手写数字识别）" class="headerlink" title="Exercise（手写数字识别）"></a>Exercise（手写数字识别）</h2><h3 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h3><ul>
<li>一维模型（一对多分类）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">    y_matrix.append((raw_y == k).astype(int))</span><br></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9alf9j5qwj31360qvq4o.jpg" alt="向量化标签.png"></p>
<p>采用LR，一次只能在2个类之间进行分类</p>
<ul>
<li>k维模型</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">k_theta = np.array([logistic_regression(X, y[k]) <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">10</span>)])</span><br><span class="line">prob_matrix = sigmoid(X @ k_theta.T)</span><br><span class="line">y_pred = np.argmax(prob_matrix, axis=<span class="number">1</span>) <span class="comment">#返回沿轴axis最大值的索引，axis=1代表行</span></span><br><span class="line">y_answer = raw_y.copy()</span><br><span class="line">y_answer[y_answer==<span class="number">10</span>] = <span class="number">0</span>  <span class="comment"># 之前已将标签为10移至第1列</span></span><br><span class="line">print(classification_report(y_answer, y_pred))</span><br></pre></td></tr></table></figure>
<p>可以进行所有标签预测</p>
<ul>
<li>前馈预测（默认已有weight和b矩阵）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_weight</span><span class="params">(path)</span>:</span></span><br><span class="line">    data = sio.loadmat(path)</span><br><span class="line">    <span class="keyword">return</span> data[<span class="string">'Theta1'</span>], data[<span class="string">'Theta2'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(path)</span>:</span></span><br><span class="line">    data = sio.loadmat(path)</span><br><span class="line">    y = data.get(<span class="string">'y'</span>)  <span class="comment"># (5000,1)</span></span><br><span class="line">    y = y.reshape(y.shape[<span class="number">0</span>])  <span class="comment"># make it back to column vector</span></span><br><span class="line"></span><br><span class="line">    X = data.get(<span class="string">'X'</span>)  <span class="comment"># (5000,400)</span></span><br><span class="line">    <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">theta1, theta2 = load_weight(<span class="string">'ex3weights.mat'</span>)</span><br><span class="line"></span><br><span class="line">X, y = load_data(<span class="string">'ex3data1.mat'</span>)</span><br><span class="line"></span><br><span class="line">X = np.insert(X, <span class="number">0</span>, values=np.ones(X.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)  <span class="comment"># intercept</span></span><br><span class="line">a1 = X</span><br><span class="line">z2 = a1 @ theta1.T  <span class="comment"># (5000, 401) @ (25,401).T = (5000, 25)</span></span><br><span class="line">z2 = np.insert(z2, <span class="number">0</span>, values=np.ones(z2.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)</span><br><span class="line">a2 = sigmoid(z2)</span><br><span class="line">z3 = a2 @ theta2.T</span><br><span class="line">a3 = sigmoid(z3)</span><br><span class="line">y_pred = np.argmax(a3, axis=<span class="number">1</span>) + <span class="number">1</span>   <span class="comment"># numpy is 0 base index, +1 for matlab convention</span></span><br><span class="line"></span><br><span class="line">print(classification_report(y, y_pred))</span><br></pre></td></tr></table></figure>
<h3 id="反向传播-1"><a href="#反向传播-1" class="headerlink" title="反向传播"></a>反向传播</h3><p>首先进行<strong>正向传播</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_forward</span><span class="params">(theta, X)</span>:</span></span><br><span class="line">    <span class="string">"""apply to architecture 400+1 * 25+1 *10</span></span><br><span class="line"><span class="string">    X: 5000 * 401</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    t1, t2 = deserialize(theta)  <span class="comment"># t1: (25,401) t2: (10,26)</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    a1 = X  <span class="comment"># 5000 * 401</span></span><br><span class="line"></span><br><span class="line">    z2 = a1 @ t1.T  <span class="comment"># 5000 * 25</span></span><br><span class="line">    a2 = np.insert(sigmoid(z2), <span class="number">0</span>, np.ones(m), axis=<span class="number">1</span>)  <span class="comment"># 5000*26</span></span><br><span class="line"></span><br><span class="line">    z3 = a2 @ t2.T  <span class="comment"># 5000 * 10</span></span><br><span class="line">    h = sigmoid(z3)  <span class="comment"># 5000*10, this is h_theta(X)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a1, z2, a2, z3, h  <span class="comment"># you need all those for backprop</span></span><br></pre></td></tr></table></figure>
<p>接着进行<strong>反向梯度下降</strong></p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9ahes3t3aj30jv05wt9q.jpg" alt="反向传播2.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 反向传播梯度下降</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    <span class="comment"># initialize</span></span><br><span class="line">    t1, t2 = deserialize(theta)  <span class="comment"># t1: (25,401) t2: (10,26)</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    delta1 = np.zeros(t1.shape)  <span class="comment"># (25, 401)</span></span><br><span class="line">    delta2 = np.zeros(t2.shape)  <span class="comment"># (10, 26)</span></span><br><span class="line"></span><br><span class="line">    a1, z2, a2, z3, h = feed_forward(theta, X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        a1i = a1[i, :]  <span class="comment"># (1, 401)</span></span><br><span class="line">        z2i = z2[i, :]  <span class="comment"># (1, 25)</span></span><br><span class="line">        a2i = a2[i, :]  <span class="comment"># (1, 26)</span></span><br><span class="line"></span><br><span class="line">        hi = h[i, :]    <span class="comment"># (1, 10)</span></span><br><span class="line">        yi = y[i, :]    <span class="comment"># (1, 10)</span></span><br><span class="line"></span><br><span class="line">        d3i = hi - yi  <span class="comment"># (1, 10)    </span></span><br><span class="line"></span><br><span class="line">        z2i = np.insert(z2i, <span class="number">0</span>, np.ones(<span class="number">1</span>))  <span class="comment"># make it (1, 26) to compute d2i</span></span><br><span class="line">        d2i = np.multiply(t2.T @ d3i, sigmoid_gradient(z2i))  <span class="comment"># (1, 26)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># careful with np vector transpose</span></span><br><span class="line">        delta2 += np.matrix(d3i).T @ np.matrix(a2i)  <span class="comment"># (1, 10).T @ (1, 26) -&gt; (10, 26)</span></span><br><span class="line">        delta1 += np.matrix(d2i[<span class="number">1</span>:]).T @ np.matrix(a1i)  <span class="comment"># (1, 25).T @ (1, 401) -&gt; (25, 401)</span></span><br><span class="line"></span><br><span class="line">    delta1 = delta1 / m</span><br><span class="line">    delta2 = delta2 / m</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> serialize(delta1, delta2)</span><br></pre></td></tr></table></figure>
<p><strong>注：</strong>$d3i$即为输出层误差，$d2i$为隐含层误差</p>
<p>这里返回的delta1，delta2是C对w求偏导的结果，这里的w包括b</p>
<h3 id="梯度校验"><a href="#梯度校验" class="headerlink" title="梯度校验"></a>梯度校验</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_checking</span><span class="params">(theta, X, y, epsilon, regularized=False)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">a_numeric_grad</span><span class="params">(plus, minus, regularized=False)</span>:</span></span><br><span class="line">        <span class="string">"""calculate a partial gradient with respect to 1 theta"""</span></span><br><span class="line">        <span class="keyword">if</span> regularized:</span><br><span class="line">            <span class="keyword">return</span> (regularized_cost(plus, X, y) - regularized_cost(minus, X, y)) / (epsilon * <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (cost(plus, X, y) - cost(minus, X, y)) / (epsilon * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    theta_matrix = expand_array(theta)  <span class="comment"># expand to (10285, 10285)</span></span><br><span class="line">    epsilon_matrix = np.identity(len(theta)) * epsilon</span><br><span class="line"></span><br><span class="line">    plus_matrix = theta_matrix + epsilon_matrix</span><br><span class="line">    minus_matrix = theta_matrix - epsilon_matrix</span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculate numerical gradient with respect to all theta</span></span><br><span class="line">    numeric_grad = np.array([a_numeric_grad(plus_matrix[i], minus_matrix[i], regularized)</span><br><span class="line">                                    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(theta))])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># analytical grad will depend on if you want it to be regularized or not</span></span><br><span class="line">    analytic_grad = regularized_gradient(theta, X, y) <span class="keyword">if</span> regularized <span class="keyword">else</span> gradient(theta, X, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If you have a correct implementation, and assuming you used EPSILON = 0.0001</span></span><br><span class="line">    <span class="comment"># the diff below should be less than 1e-9</span></span><br><span class="line">    <span class="comment"># this is how original matlab code do gradient checking</span></span><br><span class="line">    diff = np.linalg.norm(numeric_grad - analytic_grad) / np.linalg.norm(numeric_grad + analytic_grad)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">expand_array</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="string">"""replicate array into matrix</span></span><br><span class="line"><span class="string">    [1, 2, 3]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    [[1, 2, 3],</span></span><br><span class="line"><span class="string">     [1, 2, 3],</span></span><br><span class="line"><span class="string">     [1, 2, 3]]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># turn matrix back to ndarray</span></span><br><span class="line">    <span class="keyword">return</span> np.array(np.matrix(np.ones(arr.shape[<span class="number">0</span>])).T @ np.matrix(arr))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行</span></span><br><span class="line">gradient_checking(theta, X, y, epsilon= <span class="number">0.0001</span>)<span class="comment">#这个运行很慢，谨慎运行</span></span><br></pre></td></tr></table></figure>
<p>If your backpropagation implementation is correct，the relative difference will be smaller than 10e-9 (assume epsilon=0.0001)</p>
<h1 id="机器学习诊断法"><a href="#机器学习诊断法" class="headerlink" title="机器学习诊断法"></a>机器学习诊断法</h1><h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><p>一般使用60%的数据作为训练集，使用20%的数据作为<strong>交叉验证集</strong>，20%的数据作为测试集。</p>
<p>这样比只分训练集和测试集相比，可以得到更好的泛化误差。</p>
<p><strong>模型选择的方法</strong></p>
<ol>
<li>使用训练集训练出（比如）10个模型</li>
<li>用这些模型在交叉验证集中计算交叉验证误差（代价函数）</li>
<li>选取交叉验证误差最小的模型</li>
<li>利用这个模型计算测试集中的泛化误差（代价函数的值）</li>
</ol>
<p>训练误差：</p>
<p>$J_{train}(\Theta)=\frac{1}{2m}\sum_\limits{i=1}^m (h_\Theta(x^{(i)})-y^{(i)})^2$</p>
<p>交叉验证误差：</p>
<p>$J_{CV}(\Theta)=\frac{1}{2m}\sum_\limits{i=1}^m (h_\Theta(x_{CV}^{(i)})-y_{CV}^{(i)})^2$</p>
<h2 id="偏差-Bias-和方差-Variance"><a href="#偏差-Bias-和方差-Variance" class="headerlink" title="偏差(Bias)和方差(Variance)"></a>偏差(Bias)和方差(Variance)</h2><p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9g8gscmvaj30he0bn76e.jpg" alt="方差和偏差.png"></p>
<p>如何判断是欠拟合（偏差）还是过拟合（方差）：</p>
<ul>
<li><p>训练集误差和交叉验证集误差近似时：偏差/欠拟合- </p>
</li>
<li><p>交叉验证集误差&gt;&gt;训练集误差时：方差/过拟合</p>
</li>
</ul>
<h2 id="正则化-1"><a href="#正则化-1" class="headerlink" title="正则化"></a>正则化</h2><p>对于${h_\theta }\left( x \right) = {\theta _0} + {\theta _1}x + {\theta _2}{x^2} + {\theta _3}{x^3} + {\theta _4}{x^4}$</p>
<p>及$J( \theta)=\frac{1}{2m}[ {\sum\limits_{i = 1}^m ( h_\theta( x^{(i)}  - y^{(i)}) ^2+ \lambda \sum\limits_{j = 1}^n {\theta _j^2} } ]$</p>
<p><strong>选择$\lambda$的方法</strong></p>
<p>类似选择模型的方法，将训练集和交叉验证集的代价函数误差与$\lambda$的值绘制在一张图表上</p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9g8rzzas0j30fo0apjsn.jpg" alt="正则化偏差和方差.png"></p>
<ul>
<li>当$\lambda$较小时，训练集误差较小（过拟合），交叉验证集误差较大</li>
<li>随着$\lambda$的增加，训练集误差不断增加（欠拟合），而交叉验证集误差先减小后增加</li>
</ul>
<h2 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h2><p>学习曲线是学习算法的一个很好的合理检验（sanity check）。绘制训练集误差和交叉验证集误差随训练实例数量（m）的变化图</p>
<p><strong>高偏差</strong></p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9g8zh54ubj30fv09vmxw.jpg" alt="高偏差.png"></p>
<p><strong>高方差</strong></p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9g8zsog2dj30db09374y.jpg" alt="高方差.png"></p>
<ul>
<li>对于欠拟合，增加数据到训练集不一定有帮助</li>
<li>对于过拟合，增加更多数据到训练集可能提高算法效果</li>
</ul>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><strong>改进算法策略</strong></p>
<ol>
<li>获取更多的训练实例   —解决高方差</li>
<li>尝试减少特征的数量  —解决高方差</li>
<li>获取更多特征  —解决高偏差</li>
<li>增加多项式特征  —解决高偏差</li>
<li>减少正则化程度$\lambda$  —解决高偏差</li>
<li>增加正则化程度$\lambda$  —解决高方差</li>
</ol>
<h3 id="神经网络中的偏差和方差"><a href="#神经网络中的偏差和方差" class="headerlink" title="神经网络中的偏差和方差"></a>神经网络中的偏差和方差</h3><ul>
<li>使用较小的神经网络，容易导致高偏差和欠拟合</li>
<li>使用较大地神经网络，容易导致高方差和过拟合</li>
</ul>
<p>通常选择较大的神经网络并采用正则化处理，这样比使用较小网络效果要好</p>
<p>选择隐含层的方法和选择模型的方法类似。</p>
<h1 id="机器学习系统设计"><a href="#机器学习系统设计" class="headerlink" title="机器学习系统设计"></a>机器学习系统设计</h1><h2 id="学习算法的推荐方法"><a href="#学习算法的推荐方法" class="headerlink" title="学习算法的推荐方法"></a><strong>学习算法的推荐方法</strong></h2><ol>
<li>简答快速的实现一个算法，并用交叉验证集数据测试这个算法</li>
<li>绘制学习曲线，决定是增加更多数据，特征还是其他</li>
<li>进行<strong>误差分析</strong>：人工检查交叉验证集中我们算法产生预测误差的实例，看看这些实例是否有某种系统化变化的趋势</li>
</ol>
<h2 id="类偏斜"><a href="#类偏斜" class="headerlink" title="类偏斜"></a>类偏斜</h2><p>类偏斜（skewed classes）：训练集中有非常多的同一种类的实例，只有很少或没有其他类的实例</p>
<p><strong>分类问题指标</strong></p>
<ul>
<li>真阳性 TP : 预测为正，实际为正</li>
<li>假阳性 FP：预测为正，实际为负</li>
<li>假阴性 FN：预测为负，实际为正</li>
<li>真阴性 TN：预测为负，实际为负</li>
</ul>
<p><strong>查准率</strong>：预测为正的样本中有多少正样本</p>
<p>$P=\frac{TP}{TP+FP}$</p>
<p><strong>查全率：</strong>样本中的正例有多少被预测正确了</p>
<p>$R=\frac{TP}{TP+FN}$</p>
<p><strong>F1-Score</strong></p>
<p>衡量二分类模型精确度的一种指标</p>
<p>$F_1=2·\frac{precision·recall}{precision+recall}$</p>
<h2 id="机器学习数据"><a href="#机器学习数据" class="headerlink" title="机器学习数据"></a>机器学习数据</h2><p>得到一个性能很好的学习算法思路：</p>
<ul>
<li>特征值有足够多信息，这样可以保证低偏差</li>
</ul>
<h1 id="支持向量机（SVM"><a href="#支持向量机（SVM" class="headerlink" title="支持向量机（SVM)"></a>支持向量机（SVM)</h1><h2 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h2><p>在逻辑回归中，对于一个样本（x,y)来说，它的损失函数为：$-ylog(\frac{1}{1+e^{-\theta^Tx}})-(1-y)log(1-\frac{1}{1+e^{-\theta^Tx}})$</p>
<ul>
<li>对于y=1的情况，损失函数变为$-log(\frac{1}{1+e^{-z}})$</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9hdw1lzgnj30f30ad0tt.jpg" alt="y=1.png"></p>
<p>支持向量机将绿线替换成红线，称改变后的损失函数为：$Cost_1(z)$</p>
<ul>
<li>对于y=0的情况</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9hdyurbzcj30fk0ajgmy.jpg" alt="y=0.png"></p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9he0grto0j30l30blwf8.jpg" alt="svm假设函数.png"></p>
<p>因此，支持向量机的目标是：</p>
<p>$\underbrace{min}_\theta\left\{C\left[\sum_\limits{i=1}^my^{(i)}Cost_1\left(\theta^Tx^{(i)}\right)+\left(1-y^{(i)}\right)Cost_0\left(\theta^Tx^{(i)}\right)\right]+\frac{1}{2}\sum_\limits{j=1}^n\theta_j^2\right\}$</p>
<p>其中，假设函数为：</p>
<p>$h_\theta(x)=\begin{cases}1 &amp; \theta^Tx\geq0\\0 &amp; \theta^Tx&lt;0\end{cases}$</p>
<h2 id="大间距分类器（Large-Margin-Classifier"><a href="#大间距分类器（Large-Margin-Classifier" class="headerlink" title="大间距分类器（Large Margin Classifier)"></a>大间距分类器（Large Margin Classifier)</h2><p>支持向量机的<strong>目标</strong>是：</p>
<p>$\underbrace{min}_\theta\left\{C\left[\sum_\limits{i=1}^my^{(i)}Cost_1\left(\theta^Tx^{(i)}\right)+\left(1-y^{(i)}\right)Cost_0\left(\theta^Tx^{(i)}\right)\right]+\frac{1}{2}\sum_\limits{j=1}^n\theta_j^2\right\}$</p>
<p>下图分别是$Cost_1(z)$和$Cost_0(z)$的示意图：</p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9hfk2oj4pj30fu06874o.jpg" alt="svm cost.png"></p>
<p>如果y=1，我们想要$\theta^Tx\geq1$（而不仅仅是$\theta^Tx\geq0)$</p>
<p>如果y=0，我们想要$\theta^Tx\leq-1$（而不仅仅是$\theta^Tx&lt;0)$</p>
<p><strong>参数C对decision boundary的影响</strong></p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9hfy4wretj30cs09a0tw.jpg" alt="svm边界.png"></p>
<p>当C非常大时，会得到黄色的线，此时过拟合</p>
<p>当C不是适当时，它可以忽略掉一些异常点的影响，得到更好的决策界</p>
<p><strong>大间距分类器的原因</strong></p>
<p>对于代价目标的后半部分：$\frac{1}{2}\sum_\limits{j=1}^n\theta_j^2$=$\frac{1}{2}\parallel\theta\parallel^2$</p>
<p>当y=1时，$\theta^Tx$会朝着$\theta^Tx\geq1$的趋势去优化$\theta$，加上第二部分的$\theta$尽量小，则对于</p>
<p>$\theta^Tx^{(i)}=p^{(i)}\parallel\theta\parallel$，$p^{(i)}$需要尽量大</p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9ie3fs8a6j30ei05k3yv.jpg" alt="大间距分类器.png"></p>
<p>支持向量机会选择后者，因为它有较大的p</p>
<h2 id="内核"><a href="#内核" class="headerlink" title="内核"></a>内核</h2><p><strong>高斯内核</strong></p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9ieg65ga3j30ag04jglo.jpg" alt="高斯内核.png"></p>
<p>这里的”similarity”函数就是“内核”，这种内核又称为“<strong>高斯内核</strong>”</p>
<p>当x距离$l^{(1)}$越近，$f_1$越接近于1，越远越接近于0</p>
<p><strong>选定l</strong></p>
<p>通常根据训练集的数量选择地标的数量，即如果训练集中有m个实例，则选取m个地标。并且令：$l^{(1)}=x^{(1)}$，…，$l^{(m)}=x^{(m)}$。现在支持向量机的任务是：</p>
<p>$\underbrace{min}_\theta\left\{C\left[\sum_\limits{i=1}^my^{(i)}Cost_1\left(\theta^Tf^{(i)}\right)+\left(1-y^{(i)}\right)Cost_0\left(\theta^Tf^{(i)}\right)\right]+\frac{1}{2}\sum_\limits{j=1}^n\theta_j^2\right\}$</p>
<p>并且对正则化做出调整，计算$\sum_\limits{j=1}^{n=m}\theta^2=\theta^T\theta$时，用$\theta^TM\theta$代替$\theta^T\theta$，其中M是根据我们选择的核函数而不同的一个矩阵，这样做简化了计算。</p>
<p><strong>参数影响</strong></p>
<ul>
<li>C较大，过拟合，高方差</li>
<li>C较小，欠拟合，高偏差</li>
<li>$\sigma$较大，低方差，高偏差</li>
<li>$\sigma$较小，低偏差，高方差</li>
</ul>
<h2 id="SVM使用"><a href="#SVM使用" class="headerlink" title="SVM使用"></a>SVM使用</h2><p><strong>准则</strong></p>
<p>n为特征数，m为训练样本数。</p>
<ul>
<li>n比m大很多，选用逻辑回归模型或者线性支持向量机</li>
<li>n较小，m大小中等，比如n在1-1000之间，m在10-10000之间，使用高斯内核的向量机</li>
<li>n较小，m较大，n在1-1000之间，m&gt;50000，使用支持向量机会很慢，解决方案：创造增加更多的特征，然后使用逻辑回归或者线性内核</li>
<li>一般，支持向量机比神经网络快，因为SVM的代价函数是凸函数，不存在局部最小值</li>
</ul>
<h2 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h2><h3 id="线性内核"><a href="#线性内核" class="headerlink" title="线性内核"></a>线性内核</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> sklearn.svm</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mat = sio.loadmat(<span class="string">'./data/ex6data1.mat'</span>)</span><br><span class="line">data = pd.DataFrame(mat.get(<span class="string">'X'</span>), columns=[<span class="string">'X1'</span>, <span class="string">'X2'</span>])</span><br><span class="line">data[<span class="string">'y'</span>] = mat.get(<span class="string">'y'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># try C=1</span></span><br><span class="line">svc1 = sklearn.svm.LinearSVC(C=<span class="number">1</span>, loss=<span class="string">'hinge'</span>)</span><br><span class="line">svc1.fit(data[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]], data[<span class="string">'y'</span>])</span><br><span class="line"><span class="comment"># 类别预测的置信水平，这是该点与超平面距离的函数。</span></span><br><span class="line">data[<span class="string">'SVM1 Confidence'</span>] = svc1.decision_function(data[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]])</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">ax.scatter(data[<span class="string">'X1'</span>], data[<span class="string">'X2'</span>], s=<span class="number">50</span>, c=data[<span class="string">'SVM1 Confidence'</span>], cmap=<span class="string">'RdBu'</span>)</span><br><span class="line">ax.set_title(<span class="string">'SVM (C=1) Decision Confidence'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># try C=100</span></span><br><span class="line"><span class="comment"># 此时过拟合，decision_function的值变大</span></span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：svc.decision_function含义，C的取值影响</p>
<h3 id="Gaussion-kernels"><a href="#Gaussion-kernels" class="headerlink" title="Gaussion kernels"></a>Gaussion kernels</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"></span><br><span class="line"><span class="comment"># kernek function 高斯核函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian_kernel</span><span class="params">(x1, x2, sigma)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.exp(- np.power(x1 - x2, <span class="number">2</span>).sum() / (<span class="number">2</span> * (sigma ** <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mat = sio.loadmat(<span class="string">'./data/ex6data2.mat'</span>)</span><br><span class="line">data = pd.DataFrame(mat.get(<span class="string">'X'</span>), columns=[<span class="string">'X1'</span>, <span class="string">'X2'</span>])</span><br><span class="line">data[<span class="string">'y'</span>] = mat.get(<span class="string">'y'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># matplotlib画出分类图</span></span><br><span class="line"><span class="comment"># positive = data[data['y'].isin([1])]</span></span><br><span class="line"><span class="comment"># negative = data[data['y'].isin([0])]</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># fig, ax = plt.subplots(figsize=(12,8))</span></span><br><span class="line"><span class="comment"># ax.scatter(positive['X1'], positive['X2'], s=30, marker='x', label='Positive')</span></span><br><span class="line"><span class="comment"># ax.scatter(negative['X1'], negative['X2'], s=30, marker='o', label='Negative')</span></span><br><span class="line"><span class="comment"># ax.legend()</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将使用内置的RBF内核构建支持向量机分类器，并检查其对训练数据的准确性</span></span><br><span class="line">svc = svm.SVC(C=<span class="number">100</span>, kernel=<span class="string">'rbf'</span>, gamma=<span class="number">10</span>, probability=<span class="literal">True</span>)</span><br><span class="line">svc.fit(data[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]], data[<span class="string">'y'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># the predict_proba显示类别，是positive还是negative</span></span><br><span class="line">predict_prob = svc.predict_proba(data[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]])[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">ax.scatter(data[<span class="string">'X1'</span>], data[<span class="string">'X2'</span>], s=<span class="number">30</span>, c=predict_prob, cmap=<span class="string">'Reds'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>注意：画图方法，svc.predict_proba含义</p>
<h3 id="选择参数"><a href="#选择参数" class="headerlink" title="选择参数"></a>选择参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"></span><br><span class="line">mat = sio.loadmat(<span class="string">'./data/ex6data3.mat'</span>)</span><br><span class="line">training = pd.DataFrame(mat.get(<span class="string">'X'</span>), columns=[<span class="string">'X1'</span>, <span class="string">'X2'</span>])</span><br><span class="line">training[<span class="string">'y'</span>] = mat.get(<span class="string">'y'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉验证集</span></span><br><span class="line">cv = pd.DataFrame(mat.get(<span class="string">'Xval'</span>), columns=[<span class="string">'X1'</span>, <span class="string">'X2'</span>])</span><br><span class="line">cv[<span class="string">'y'</span>] = mat.get(<span class="string">'yval'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># manual grid search</span></span><br><span class="line">candidate = [<span class="number">0.01</span>, <span class="number">0.03</span>, <span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">30</span>, <span class="number">100</span>]</span><br><span class="line">combination = [(C, gamma) <span class="keyword">for</span> C <span class="keyword">in</span> candidate <span class="keyword">for</span> gamma <span class="keyword">in</span> candidate]</span><br><span class="line">search = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> C, gamma <span class="keyword">in</span> combination:</span><br><span class="line">    svc = svm.SVC(C=C, gamma=gamma)</span><br><span class="line">    svc.fit(training[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]], training[<span class="string">'y'</span>])</span><br><span class="line">    search.append(svc.score(cv[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]], cv[<span class="string">'y'</span>]))</span><br><span class="line"></span><br><span class="line">best_score = search[np.argmax(search)]</span><br><span class="line">best_param = combination[np.argmax(search)]</span><br><span class="line"></span><br><span class="line">best_svc = svm.SVC(C=<span class="number">100</span>, gamma=<span class="number">0.3</span>)</span><br><span class="line">best_svc.fit(training[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]], training[<span class="string">'y'</span>])</span><br><span class="line">ypred = best_svc.predict(cv[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]])</span><br><span class="line"></span><br><span class="line">print(metrics.classification_report(cv[<span class="string">'y'</span>], ypred))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># sklearn GridSearchCV</span></span><br><span class="line">parameters = &#123;<span class="string">'C'</span>: candidate, <span class="string">'gamma'</span>: candidate&#125;</span><br><span class="line">svc = svm.SVC()</span><br><span class="line">clf = GridSearchCV(svc, parameters, n_jobs=<span class="number">-1</span>)</span><br><span class="line">clf.fit(training[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]], training[<span class="string">'y'</span>])</span><br><span class="line">ypred = clf.predict(cv[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]])</span><br><span class="line">print(metrics.classification_report(cv[<span class="string">'y'</span>], ypred))</span><br></pre></td></tr></table></figure>
<p> 自动搜索与人工搜索搜索得到<strong>参数不同</strong>的原因:<br> 自动搜索只使用了部分training data,把剩余training当做CV.</p>
<h3 id="垃圾邮件分类"><a href="#垃圾邮件分类" class="headerlink" title="垃圾邮件分类"></a>垃圾邮件分类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"></span><br><span class="line">mat_tr = sio.loadmat(<span class="string">'data/spamTrain.mat'</span>)</span><br><span class="line">X, y = mat_tr.get(<span class="string">'X'</span>), mat_tr.get(<span class="string">'y'</span>).ravel()</span><br><span class="line"></span><br><span class="line">mat_test = sio.loadmat(<span class="string">'data/spamTest.mat'</span>)</span><br><span class="line">test_X, test_y = mat_test.get(<span class="string">'Xtest'</span>), mat_test.get(<span class="string">'ytest'</span>).ravel()</span><br><span class="line"></span><br><span class="line">svc = svm.SVC()</span><br><span class="line">svc.fit(X, y)</span><br><span class="line"></span><br><span class="line">pred = svc.predict(test_X)</span><br><span class="line">print(metrics.classification_report(test_y, pred))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 逻辑回归试试</span></span><br><span class="line">logit = LogisticRegression()</span><br><span class="line">logit.fit(X, y)</span><br><span class="line">pred = logit.predict(test_X)</span><br><span class="line">print(metrics.classification_report(test_y, pred))</span><br></pre></td></tr></table></figure>
<h1 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h1><h2 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h2><h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><p>随机选择K个聚类中心</p>
<p>重复{</p>
<ol>
<li>将所有样本点划分在这K类中</li>
<li>在每个类中重新计算聚类中心</li>
</ol>
<p>}直至聚类中心不再变化</p>
<h3 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h3><p>符号：</p>
<p>$c^{(i)}$—样本$x^{(i)}$当前所属的类</p>
<p>$\mu_k$—第k个类别的中心</p>
<p>$\mu_c^{(i)}$—样本x(i)所属类的聚类中心</p>
<p>目标：$\underbrace \min _{c^{( 1 )},…,c^{( m )},\mu _1,…,\mu _K}J( c^{( 1 )},…,c^{( m )},\mu _1,…,\mu _K ) = \frac{1}{m}\sum\limits_{i = 1}^m \parallel x^{(i)}-\mu_c^{(i)} \parallel^2$</p>
<h3 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h3><p>通常多次运行K-means算法，选取代价函数最小的结果。当K=(2-10)时可行。若K值较大，这么做也可能不会有明显改善</p>
<h3 id="K值选择"><a href="#K值选择" class="headerlink" title="K值选择"></a>K值选择</h3><ul>
<li>“肘部法则”</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9ii4rukprj309f060q3a.jpg" alt="肘部法则.png"></p>
<ul>
<li>根据聚类后的“后续目的”人工选择</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/15/Tensorflow2.0与深度学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kason">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/westbrook.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kason's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/15/Tensorflow2.0与深度学习/" itemprop="url">Tensorflow2.0</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-15T18:02:28+08:00">
                2019-11-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/11/15/Tensorflow2.0与深度学习/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/11/15/Tensorflow2.0与深度学习/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Tensorflow-基础"><a href="#Tensorflow-基础" class="headerlink" title="Tensorflow 基础"></a>Tensorflow 基础</h1><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><h3 id="数值类型"><a href="#数值类型" class="headerlink" title="数值类型"></a>数值类型</h3><p>在Tensorflow中，为了表示方便，一般把标量、向量、矩阵统称为<strong>张量（Tensor）</strong></p>
<ul>
<li>通过打印张量信息，得到</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [2]: x = tf.constant([1,2.,3.3])</span><br><span class="line">x</span><br><span class="line"></span><br><span class="line">Out[2]:</span><br><span class="line">&lt;tf.Tensor: id=165, shape=(3,), dtype=float32, numpy=array([1. , 2. , 3.3],</span><br><span class="line">dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>其中id是TensorFlow中内部索引对象的编号，shape表示张量的形状，dtype表示张量的精度，张量numpy（）方法可以返回Numpy.array类型的数据</li>
<li>x.numpy() 返回Numpy.array类型的数据</li>
<li>tf.constant([[1,2], [3,4]])  定义2维张量</li>
</ul>
<h3 id="字符串类型"><a href="#字符串类型" class="headerlink" title="字符串类型"></a>字符串类型</h3><p>提供常见的join()，length()，split()，lower()等工具函数</p>
<h3 id="布尔类型"><a href="#布尔类型" class="headerlink" title="布尔类型"></a>布尔类型</h3><ul>
<li>Tensorflow的布尔类型和Python语言的布尔类型不对等</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [11]:</span><br><span class="line">a = tf.constant(True) # 创建布尔张量</span><br><span class="line">a == True</span><br><span class="line"></span><br><span class="line">Out[11]:</span><br><span class="line">False</span><br></pre></td></tr></table></figure>
<h2 id="数值精度"><a href="#数值精度" class="headerlink" title="数值精度"></a>数值精度</h2>
          
        
      
    </div>
    
    
    

    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/13/Learning Geo-Social User Topical Profiles with Bayesian Hierarchical User Factorization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kason">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/westbrook.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kason's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/13/Learning Geo-Social User Topical Profiles with Bayesian Hierarchical User Factorization/" itemprop="url">Learning Geo-Social User Topical Profiles with Bayesian Hierarchical User Factorization</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-13T20:25:47+08:00">
                2019-11-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/论文合集/" itemprop="url" rel="index">
                    <span itemprop="name">论文合集</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/11/13/Learning Geo-Social User Topical Profiles with Bayesian Hierarchical User Factorization/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/11/13/Learning Geo-Social User Topical Profiles with Bayesian Hierarchical User Factorization/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h1><p> 使用<strong>贝叶斯分层用户分解</strong>学习地理-社交用户主题画像 </p>
<h2 id="Research-Objective"><a href="#Research-Objective" class="headerlink" title="Research Objective"></a>Research Objective</h2><p>在社会空间系统中，实现地理位置和用户画像的交互</p>
<p>给出已观测到的用户地理画像，对user的tag-location进行ranking预测未知的画像</p>
<h2 id="Background-and-Problems"><a href="#Background-and-Problems" class="headerlink" title="Background and Problems"></a>Background and Problems</h2><ol>
<li><p>虽然很多现有用户画像关注的每个用户的全局视角，但仍有一些重要的地缘社会因素需要考虑</p>
<ul>
<li><p>每个用户在不同地方被不同感知</p>
</li>
<li><p>具有相似画像的用户可能有巨大的地缘影响差异</p>
</li>
</ul>
</li>
<li><strong>建模地理画像挑战</strong></li>
</ol>
<p>可以采用BPTF（贝叶斯泊松张量分解）进行预测，但存在如下问题：</p>
<ul>
<li>经常分散，由于用户异质性，地理画像中的受欢迎程度计数存在很大差异</li>
<li><p>由于多维性，通常非常稀疏</p>
<ol>
<li>先前研究</li>
</ol>
</li>
<li><p>先前研究大多集中在揭示用户的主题画像或潜在兴趣，没有明确考虑地缘社会因素。</p>
</li>
<li>新兴的研究方向侧重于用Gamma-Poisson分布代替传统的高斯分布对离散数据建模</li>
<li>利用各种背景信号来改善学习。包括文本、社交网络等</li>
</ul>
<h2 id="Method-s"><a href="#Method-s" class="headerlink" title="Method(s)"></a>Method(s)</h2><ol>
<li>为了克服异质性，提出了一个两层的贝叶斯层级用户分解生成框架（bHUF），该框架很容易推广到深度用户分解。</li>
<li>为了减少稀疏性，研究用户上下文（特别是地理位置和社会关系），针对多层因式分解方案的非共轭性,提出一个增强模型（bHUF+）。然后，使用NB分布的数据增强方案，开发一种有效的封闭式<strong>吉布斯抽样</strong>方案进行推理</li>
</ol>
<ul>
<li>两层贝叶斯分层用户分解，将泊松伽马信念网络从二维非负计数推广到多维异构计数。与单层分解相比，用户分解的额外层通过允许更大的用户地理位置分布方差-均值比，比单层具有的学习。更好地处理过度分散和用户异质性。</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>作者给了哪些结论，哪些是strong conclusions,哪些是weak的conclusions？哪些构思？</p>
<p>根据GPS标记的Twitter数据集上的几个基线，对bHUF和bHUF+进行了评估，观察到bHUF在最佳替代单层基线的精确度和召回率上提高了约5%~13%，对用户地理位置和社会环境的改善率提高了6%~11% </p>
<h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>作者如何评估自己的方法，实验的setup是什么样的，有没有问题或者可以借鉴的地方</p>
<p>数据来源+重要指标+模型步骤+每个步骤得出的结论</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>写完笔记之后最后填，概述文章的内容。切记需要通过自己的思考，用自己的语言描述</p>
<h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><p>额外笔记</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>(optional)列出相关性高的文献，一遍之后可以继续track</p>

          
        
      
    </div>
    
    
    

    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/13/推荐系统可解释性/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kason">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/westbrook.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kason's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/13/推荐系统可解释性/" itemprop="url">推荐系统可解释性</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-13T19:03:58+08:00">
                2019-11-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/" itemprop="url" rel="index">
                    <span itemprop="name">推荐系统</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/11/13/推荐系统可解释性/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/11/13/推荐系统可解释性/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><strong>解释的两个维度：</strong></p>
<ul>
<li>显示样式（display style）：<ul>
<li>相关用户或项目</li>
<li>用户或项目特征</li>
<li>文本解释</li>
<li>图像解释</li>
<li>社会解释</li>
<li>词簇</li>
</ul>
</li>
<li>模型/方法<ul>
<li>基于邻域    </li>
<li>矩阵分解</li>
<li>主题建模</li>
<li>基于图</li>
<li>深度学习</li>
<li>知识图谱</li>
<li>关联分析</li>
</ul>
</li>
</ul>
<p><strong>两种可解释性：</strong></p>
<ol>
<li>以人为方式工作的可解释模型。目前大多数属于这一类</li>
<li>只关注产品，将推荐模型视为一个复杂的黑盒。称为事后可解释性。</li>
</ol>
<p><strong>可解释性与时效性：</strong></p>
<p>近些年的研究表明，这两项并不冲突，可以使用深度学习等方法来解决。</p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9p9hjwjl9j30qx0gs77g.jpg" alt="推荐解释.png"></p>
<h1 id="不同显示风格的解释"><a href="#不同显示风格的解释" class="headerlink" title="不同显示风格的解释"></a>不同显示风格的解释</h1><p><strong>基于相关用户和项目的解释</strong></p>
<p>协同过滤和基于相关商品的解释</p>
<p>存在信任性和可依赖性问题，因为我并不认识相似用户。</p>
<p><strong>基于特征的解释</strong></p>
<p>基于特征与基于内容密切相关，推荐与用户目标文件匹配的项目特征作为解释</p>
<ol>
<li>采用标签解释</li>
<li>雷达图</li>
</ol>
<p><strong>基于文本解释</strong></p>
<p>可以分为两个方面</p>
<ul>
<li>Aspect-level 类似基于功能的解释，通常不直接提供aspect，而是从用户意见、评论中提取。</li>
</ul>
<ol>
<li><p>为了提取aspect，采用 方面-观点-情感三元组</p>
</li>
<li><p>或者采用主题建模的方法。生成词云，从几个角度进行推荐</p>
</li>
</ol>
<ul>
<li>sentence-level</li>
</ul>
<ol>
<li><p>可以基于模板，生成句子进行推荐。</p>
</li>
<li><p>无模板，基于自然语言生成模型直接生成。将众包和计算相结合，生成个性化的自然语言解释</p>
</li>
</ol>
<p><strong>图像解释</strong></p>
<p>通过整个图像或图像中特定视觉亮点作为解释</p>
<ol>
<li>运用于衣服推荐（对某种样式特别钟爱）等，目前处于起步阶段。</li>
</ol>
<p><strong>社会解释</strong></p>
<p>依据目标用户的社交关系提供解释，有助于提高用户对推荐的信任</p>
<ol>
<li>地理-社会推荐</li>
<li>基于深交网络的小组推荐</li>
</ol>
<h1 id="推荐解释模型"><a href="#推荐解释模型" class="headerlink" title="推荐解释模型"></a>推荐解释模型</h1><h2 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h2><ol>
<li>Explicit Factor Models(EFM)  推荐用户关心的特征表现好的产品</li>
<li>学习基于张量分解学习对可解释推荐的特征进行排序</li>
<li>张量分解多任务学习，“用户偏好建模”和“有针对性的内容建模”被整合</li>
<li>Explainable Matrix Factorization(EMF) 基于用户项评级矩阵提供可解释的建议</li>
</ol>
<h2 id="主题建模"><a href="#主题建模" class="headerlink" title="主题建模"></a>主题建模</h2><p>基于可用的文本信息。主题建模通常以主题词云的形式为用户提供直观的解释</p>
<ol>
<li>FLAME model(Factorized Latent Aspect Model) 了解用户对商品不同方面的不同看法。 在词云中显示与其情感成正比的方面进行解释</li>
<li>同时利用社会评论和可信赖的社会关系来改善评分预测</li>
</ol>
<h2 id="基于图的推荐解释"><a href="#基于图的推荐解释" class="headerlink" title="基于图的推荐解释"></a>基于图的推荐解释</h2><ol>
<li>在top-N推荐中引入 user-item-aspect三元关系。对三元图的顶点进行排名。</li>
<li>不使用外部信息下，基于用户-项目聚类得到推荐。</li>
</ol>
<h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><ol>
<li>在文本评论中使用CNN对用户的偏好和商品属性进行建模。有选择地从具有不同注意权重的评论中选择单词，通过获得的注意权重，指出哪个部分更重要。可以突出相关词作为解释。</li>
<li>基于字符级RNN结构自动生成自然语言解释。</li>
<li>结合众包和计算的过程，生成个性化自然语言解释</li>
<li>为突出显示的图像区域生成自然语言解释</li>
<li>基于自然语言生成的可解释推荐</li>
</ol>
<h2 id="知识库嵌入"><a href="#知识库嵌入" class="headerlink" title="知识库嵌入"></a>知识库嵌入</h2><ol>
<li>构造用户-物品知识图，通过知识图找到从用户到推荐商品的最短路径来进行解释</li>
</ol>
<h2 id="关联规则挖掘"><a href="#关联规则挖掘" class="headerlink" title="关联规则挖掘"></a>关联规则挖掘</h2><h2 id="事后解释"><a href="#事后解释" class="headerlink" title="事后解释"></a>事后解释</h2><p>有时推荐解释不是从推荐模型本身生成的。它是在推荐模型推荐了某个项目之后，由解释模型生成的。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li>以人为方式工作的可解释模型。目前大多数属于这一类</li>
<li>只关注产品，将推荐模型视为一个复杂的黑盒。称为事后可解释性</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/13/文献阅读方法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kason">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/westbrook.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kason's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/13/文献阅读方法/" itemprop="url">文献阅读方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-13T12:19:52+08:00">
                2019-11-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/论文合集/" itemprop="url" rel="index">
                    <span itemprop="name">论文合集</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/11/13/文献阅读方法/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/11/13/文献阅读方法/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="阅读文献"><a href="#阅读文献" class="headerlink" title="阅读文献"></a>阅读文献</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ol>
<li>作者想要解决什么问题？  question</li>
<li>作者通过什么理论/模型来解决这个问题？ method</li>
<li>作者给出的答案是什么？  answer</li>
</ol>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ol>
<li>作者为什么研究这个课题？</li>
<li>目前这个课题的研究进行到了哪一个阶段？</li>
<li>作者使用的理论是基于哪些假设?</li>
</ol>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ol>
<li>这篇文章存在哪些缺陷？</li>
<li>作者关于这个课题的构思是哪几点？</li>
</ol>
<h2 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h2><p>数据来源+重要指标+模型步骤+每个步骤得出的结论</p>
<h2 id="归纳"><a href="#归纳" class="headerlink" title="归纳"></a>归纳</h2><p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g8wa9839kdj30hs0960tv.jpg" alt="文献阅读.jpg"></p>
<h2 id="笔记框架"><a href="#笔记框架" class="headerlink" title="笔记框架"></a>笔记框架</h2><blockquote>
<h1 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h1><p>文章标题</p>
<h2 id="Research-Objective"><a href="#Research-Objective" class="headerlink" title="Research Objective"></a>Research Objective</h2><p>作者的研究目标</p>
<h2 id="Background-and-Problems"><a href="#Background-and-Problems" class="headerlink" title="Background and Problems"></a>Background and Problems</h2><p>我们为什么要研究这个课题？这个课题研究进行到哪个阶段？基于哪些假设？</p>
<h2 id="Method-s"><a href="#Method-s" class="headerlink" title="Method(s)"></a>Method(s)</h2><p>作者解决问题的理论/模型？是否基于前人的方法？</p>
<h2 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>作者给了哪些结论，哪些是strong conclusions,哪些是weak的conclusions？哪些构思？</p>
<h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>作者如何评估自己的方法，实验的setup是什么样的，有没有问题或者可以借鉴的地方</p>
<p>数据来源+重要指标+模型步骤+每个步骤得出的结论</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>写完笔记之后最后填，概述文章的内容。切记需要通过自己的思考，用自己的语言描述</p>
<h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><p>额外笔记</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>(optional)列出相关性高的文献，一遍之后可以继续track</p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/18/推荐系统实践/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kason">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/westbrook.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kason's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/18/推荐系统实践/" itemprop="url">推荐系统实践</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-18T14:47:27+08:00">
                2019-10-18
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/" itemprop="url" rel="index">
                    <span itemprop="name">推荐系统</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/10/18/推荐系统实践/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/10/18/推荐系统实践/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="好的推荐系统"><a href="#好的推荐系统" class="headerlink" title="好的推荐系统"></a>好的推荐系统</h1><p>随着信息技术和互联网的发展，人们逐渐从信息匮乏的时代走入了<strong>信息过载</strong>（information overload)  。为了解决信息过载问题，出现了<em>分类目录</em>（雅虎、hao123） 和 <em>搜索引擎</em>(google) 。  </p>
<p>和搜索引擎一样，<strong>推荐系统</strong>也是一种帮助用户快速发现有用信息的工具。但是它不需要用户提供明确需求，而是通过分析用户的历史行为给用户的兴趣建模，从而主动给用户推荐。</p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g82ef90esaj30g808s40r.jpg" alt="物品和用户.png"></p>
<p>物品的<strong>长尾</strong>(long tail)<80%的销售额来自20%的热门品牌>。长尾商品代表一小部分用户的<strong>个性化需求</strong>。推荐系统通过发掘用户的行为，找到用户的个性化需求，从而将长尾商品准确地推荐给需要它的用户。</80%的销售额来自20%的热门品牌></p>
<h2 id="个性化推荐系统的应用"><a href="#个性化推荐系统的应用" class="headerlink" title="个性化推荐系统的应用"></a>个性化推荐系统的应用</h2><p>几乎所有的推荐系统都是由前台的展示页面、后台的日志系统、推荐算法系统3部分构成</p>
<h3 id="电子商务"><a href="#电子商务" class="headerlink" title="电子商务"></a>电子商务</h3><p>在亚马逊平台上  </p>
<ol>
<li>个性化推荐：基于物品、 基于好友  </li>
<li>相关推荐列表：购买（浏览）过这个商品的用户购买的其它商品，然后进行<em>打包销售</em>（cross selling)</li>
</ol>
<h3 id="电影视频、音乐推荐"><a href="#电影视频、音乐推荐" class="headerlink" title="电影视频、音乐推荐"></a>电影视频、音乐推荐</h3><p>电影视频主要通过基于物品的推荐  </p>
<p>音乐推荐是推荐系统里非常特殊的领域。具有一些特点  </p>
<ul>
<li>物品空间大  </li>
<li>听一首歌耗时很少  </li>
<li>物品重用率很高  </li>
<li>上下文相关  </li>
<li>次序很重要   </li>
<li>不需要全神贯注  </li>
<li>高度社会化</li>
</ul>
<p>以上特点决定了音乐是一种非常适合用来推荐的物品。</p>
<h3 id="社交网络"><a href="#社交网络" class="headerlink" title="社交网络"></a>社交网络</h3><p>社交网络中的个性化推荐技术主要应用在三个方面：  </p>
<ol>
<li>利用用户的社交网络信息对用户进行个性化的物品推荐  </li>
<li>信息流的会话推荐  </li>
<li>推荐好友  </li>
</ol>
<h4 id="会话推荐"><a href="#会话推荐" class="headerlink" title="会话推荐"></a>会话推荐</h4><p>用户在Facebook上的每个分享和它的所有评论被称为一个<strong>会话</strong>，如何给这些会话排序是重要的。</p>
<h3 id="个性化阅读、邮件，位置推荐"><a href="#个性化阅读、邮件，位置推荐" class="headerlink" title="个性化阅读、邮件，位置推荐"></a>个性化阅读、邮件，位置推荐</h3><h3 id="个性化广告（计算广告）"><a href="#个性化广告（计算广告）" class="headerlink" title="个性化广告（计算广告）"></a>个性化广告（计算广告）</h3><p>个性化广告与狭义个性化推荐的区别： 个性化推荐这种帮助用户找到可能令他们感兴趣的物品，而广告推荐着重帮助广告找到对他们感兴趣的用户。  </p>
<p>个性化广告投放技术主要分3种：  </p>
<ol>
<li>上下文广告  </li>
<li>搜索广告  </li>
<li>个性化展示广告：  根据用户的兴趣</li>
</ol>
<h2 id="推荐系统的评测"><a href="#推荐系统的评测" class="headerlink" title="推荐系统的评测"></a>推荐系统的评测</h2><p>一个完整的推荐系统一般存在3个参与方：用户、物品提供者和提供推荐系统的网站。推荐系统是一个三方共赢的系统</p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g82ef2mkd8j306d065mx8.jpg" alt="推荐三方.png"></p>
<h3 id="推荐系统的实验方法"><a href="#推荐系统的实验方法" class="headerlink" title="推荐系统的实验方法"></a>推荐系统的实验方法</h3><h4 id="离线实验"><a href="#离线实验" class="headerlink" title="离线实验"></a>离线实验</h4><p>步骤  </p>
<ol>
<li>通过日志系统获取用户行为数据，按照一定标准生成一个数据集  </li>
<li>将数据集划分成训练集和测试集  </li>
<li>在训练集上训练用户兴趣模型，在测试集上进行预测  </li>
<li>通过事先定义的离线指标评测算法在测试集上预测结果</li>
</ol>
<p>离线实验的优缺点如下：<br><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g82elcr60dj30nb03kmxp.jpg" alt="离线实验优缺点.png"></p>
<h4 id="用户调查"><a href="#用户调查" class="headerlink" title="用户调查"></a>用户调查</h4><p>在上线之前，需要做一次用户调查的测试。以减少直接上线的巨大风险。<br>优点是：可以获得用户主观的感受，相对在线风险低。<br>缺点是：很难组织大规模的测试用户，测试环境下的用户行为和真实环境下可能有所不同</p>
<h4 id="在线实验"><a href="#在线实验" class="headerlink" title="在线实验"></a>在线实验</h4><p><strong>AB测试</strong>是一种常见的在线评测算法的实验方法。通过一定的规则将用户随机分成几组，并对不同组的用户采用不同的算法，然后统计不同组用户的各种不同的评测指标。<br>优点：公平<br>缺点：周期长，所以一般只测试在离线实验和用户调查中表现好的算法。AB测试设计也是一项复杂的工程（一般不同层及控制这些层的团队需要从一个统一的地方获得自己AB测试的流量，不同层之间的流量应该是正交的）</p>
<p>AB测试系统：<br><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g82exdq4c6j30gj07bglu.jpg" alt="AB测试.png"></p>
<blockquote>
<p>一个新的推荐算法最终上线，需要完成以上3个实验  </p>
<ol>
<li>离线实验证明它在很多离线指标上优于现有的算法  </li>
<li>用户调查保证满意度  </li>
<li>AB测试确定优越性</li>
</ol>
</blockquote>
<h3 id="评测指标"><a href="#评测指标" class="headerlink" title="评测指标"></a>评测指标</h3><h4 id="用户满意度"><a href="#用户满意度" class="headerlink" title="用户满意度"></a>用户满意度</h4><p>用户满意度不能通过离线实验得到，只能通过用户调查或者在线实验。<br>用户调查主要使用调查问卷<br>在线实验可以通过点击率、停留时间、转化率等指标</p>
<h4 id="预测准确度"><a href="#预测准确度" class="headerlink" title="预测准确度"></a>预测准确度</h4><p>最重要的<strong>离线</strong>评测指标。准确度有几个方向 </p>
<ul>
<li>评分预测<br>一般通过均方根误差(RMSE)和平均绝对误差(RAM)计算  </li>
</ul>
<p>令$r_{ui}$是用户u对物品i的实际评分，而$\hat{r}_{ui}$是推荐算法给出的预测评分</p>
<script type="math/tex; mode=display">RMSE=\frac{\sqrt{\sum _{u,i\in T} ( r_{ui}  - \hat{r}_{ui})^2}}{|T|}</script><script type="math/tex; mode=display">RAM=\frac{\sum _{u,i\in T} | r_{ui}  - \hat{r}_{ui}|}{|T|}</script><blockquote>
<p>如果评分系统是基于整数建立的（即用户给的评分都是整数），那么对预测结果取整会降低MAE的误差</p>
</blockquote>
<ul>
<li>TopN推荐<br>TopN推荐的准确率一般通过准确率（precision)，召回率（recall)度量  </li>
</ul>
<p>令R(u)是根据用户在训练集上的行为给用户做出的推荐列表，而T(u）是用户在测试集上的行为列表</p>
<p>召回率定义为：  </p>
<script type="math/tex; mode=display">Recall=\frac{\sum_ {u\in U} | R(u) \cap T(u)|}{\sum_{u\in U} | T(u) |}</script><p>准确率定义为：</p>
<script type="math/tex; mode=display">Precision=\frac{\sum_ {u\in U} | R(u) \cap T(u)|}{\sum_{u\in U} | R(u) |}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PrecisionRecall</span><span class="params">(test, N)</span>:</span></span><br><span class="line">	hit = <span class="number">0</span></span><br><span class="line">	n_recall = <span class="number">0</span></span><br><span class="line">	n_precision = <span class="number">0</span></span><br><span class="line">	<span class="keyword">for</span> user, items <span class="keyword">in</span> test.items():</span><br><span class="line">		rank = Recommend(user, N)</span><br><span class="line">		hit += len(rank &amp; items)</span><br><span class="line">		n_recall += len(items)</span><br><span class="line">		n_precision += N</span><br><span class="line">	<span class="keyword">return</span> [hit / (<span class="number">1.0</span> * n_recall), hit / (<span class="number">1.0</span> * n_precision)]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>有时候，为了全面评测TopN推荐的准确率和召回率，会选取不同的推荐列表长度N，计算出一组准确率、召回率</p>
</blockquote>
<h4 id="覆盖率"><a href="#覆盖率" class="headerlink" title="覆盖率"></a>覆盖率</h4><p><strong>覆盖率</strong>（coverage)描述一个推荐系统对物品长尾的发掘能力。最简单的定义为推荐系统能够推荐出来的物品占总物品集合的比例。</p>
<p>假设系统的用户集合为U,推荐系统给每一个用户推荐一个长度为N的物品列表R(u)。<br>$<br>Coverage=\frac{|\cup_{u\in U}R(u)|}{|I|}<br>$</p>
<p>为了更细致地描述推荐系统发掘长尾的能力，需要统计推荐列表中不同物品出现次数的分布。如果这个分布<strong>较平</strong>，则覆盖率<strong>高</strong>。有两个指标可以用来定义覆盖率</p>
<ul>
<li>信息熵<br>概率分布越<strong>平均</strong>，<strong>信息熵越大</strong>，<strong>覆盖率越大</strong>。  <script type="math/tex; mode=display">H=-\sum_i^n p(i)\log{p(i)}</script></li>
</ul>
<p>这里p(i)是物品i的流行度除以所有物品流行度之和</p>
<blockquote>
<p>物品流行度： 有多少用户与该物品发生关系</p>
</blockquote>
<ul>
<li>基尼系数（Gini Index):<script type="math/tex; mode=display">G=\frac{1}{n-1}\sum_{j=1}^n(2j-n-1)p(i_j)</script></li>
</ul>
<p>这里，$i_j$是按照物品流行度p()从小到大排序的物品列表中第j个物品。</p>
<blockquote>
<p>基尼系数的计算原理：<br>基尼系数=SA/(SA+SB),如果系统的流行度很平均，那么SA就会很小，分配不均匀,基尼系数很大</p>
</blockquote>
<p>曲线表示最不热门的x%物品的总流行度占系统的比例y%<br><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g82q4tdgtij307c06lq2y.jpg" alt="基尼系数.png"></p>
<p>如果G1是从初始用户行为中计算出的物品流行度的基尼系数，G2是从推荐列表中计算出来的。如果G2&gt;G1,说明推荐算法具有<em>马太效应</em>。系统还需优化。</p>
<blockquote>
<p>马太效应：前者更强，弱者更弱。热门的物品更加热门，冷门更加冷门。推荐系统的初衷就是希望消除马太效应</p>
</blockquote>
<h4 id="多样性"><a href="#多样性" class="headerlink" title="多样性"></a>多样性</h4><p>假设s(i,j)∈[0,1]定义了物品i和j之间的相似度，用户u的推荐列表R(u)的多样性定义为：</p>
<script type="math/tex; mode=display">Diversity=1- \frac{\sum_ {i,j \ in R(u),i \neq j} s(i,j)}{\frac{1}{2} |R(u)| (|R(u)| -1)}</script><p>整体多样性为所有用户推荐列表多样性的平均值</p>
<p>不同物品相似度函数s(i,j)可以定义不同的多样性。如果用内容相似函数描述物品间的相似度，可以得到内容多样性函数。如果用协同过滤的相似函数，得到协同过滤的多样性函数</p>
<h4 id="新颖性"><a href="#新颖性" class="headerlink" title="新颖性"></a>新颖性</h4><p>评测新颖度的最简单的方法是利用推荐结果的平均流行度，越不热门的物品越可能让用户觉得新颖。目前的关注点在于如何在不牺牲精度的情况下提高多样性和新颖性。</p>
<p>计算平均流行度时对每个物品的流行度取对数，这是因为物品的流行度满足长尾分布，在取对数后，流行度的平均值更加稳定</p>
<blockquote>
<p>这里取对数可以使数据更加平稳，同时可以减少区间差异影响。比如log500 + log500 &gt; log200 + log800，两个都是500，相比较后者更为流行。</p>
</blockquote>
<h4 id="惊喜度"><a href="#惊喜度" class="headerlink" title="惊喜度"></a>惊喜度</h4><p>区分新颖性： 惊喜性是推荐结果和用户历史上喜欢的物品不相似，但用户觉得满意<br>新颖性：推荐用户没听说过的</p>
<h4 id="信任度"><a href="#信任度" class="headerlink" title="信任度"></a>信任度</h4><p>目前有两种方式：  </p>
<ul>
<li>增加推荐系统的透明度（transparency) ： 提供推荐解释</li>
<li>通过社交网络信息，利用好友进行解释</li>
</ul>
<h4 id="实时性"><a href="#实时性" class="headerlink" title="实时性"></a>实时性</h4><ul>
<li>推荐系统需要实时更新推荐列表</li>
<li>推荐系统需要能够将新加入系统的物品推荐给用户</li>
</ul>
<h4 id="健壮性（robust）"><a href="#健壮性（robust）" class="headerlink" title="健壮性（robust）"></a>健壮性（robust）</h4><p>除了选择健壮性高的算法，还有以下方法</p>
<ul>
<li>设计推荐系统时尽量使用代价比较高的用户行为</li>
<li>使用数据前，进行攻击检测，完成对数据的清洗</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g84ygfkwioj30lu075q3f.jpg" alt="评测指标总结.png"></p>
<p>离线实验的<strong>优化目标</strong>是：<br>最大化预测准确度   使得 覆盖率&gt;A , 多样性&gt;B , 新颖性&gt;C</p>
<h3 id="评测维度"><a href="#评测维度" class="headerlink" title="评测维度"></a>评测维度</h3><p>知道一个算法在什么情况下性能最好</p>
<ul>
<li>用户维度： 主要包括用户的人口统计学信息、活跃度以及是否为新用户</li>
<li>物品维度： 物品的属性信息、流行度、平均分、是不是新加的物品等</li>
<li>时间维度： 季节、周末还是工作日、白天还是晚上</li>
</ul>
<blockquote>
<p>在不同维度下的评测指标，可以帮助我们全面地了解推荐系统。</p>
</blockquote>
<hr>
<h1 id="利用用户行为数据"><a href="#利用用户行为数据" class="headerlink" title="利用用户行为数据"></a>利用用户行为数据</h1><p>基于用户行为分析的推荐算法是个性化推荐系统的重要算法，一般把这种类型的算法成为协同过滤算法。</p>
<h2 id="用户行为数据简介"><a href="#用户行为数据简介" class="headerlink" title="用户行为数据简介"></a>用户行为数据简介</h2><p>用户行为在个性化推荐系统中一般分两种</p>
<ul>
<li>显性反馈(explicit feedback)</li>
<li>隐性反馈(implicit feedback) </li>
</ul>
<blockquote>
<p>隐性反馈只有正反馈</p>
</blockquote>
<p>按照反馈的方向</p>
<ul>
<li>正反馈：用户的行为倾向用户喜欢该物品</li>
<li>负反馈：用户的行为倾向用户不喜欢该物品</li>
</ul>
<p>用户行为的统一表示：<br><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g84z6tto64j30lb05ewfr.jpg" alt="用户行为种类.png"></p>
<p>代表性数据集：</p>
<ul>
<li>无上下文信息的隐性反馈数据集： 仅包含用户ID和物品ID</li>
<li>无上下文信息的显性反馈数据集： 每一条记录包含用户ID、物品ID和对物品的评分</li>
<li>有上下文信息的隐性反馈数据集： 每一条记录包含用户ID、物品ID和对物品产生行为的时间戳</li>
<li>有上下文信息的显性反馈数据集： 每一条记录包括用户ID、物品ID、用户对物品的评分和评分行为发生的时间戳</li>
</ul>
<h2 id="用户行为分析"><a href="#用户行为分析" class="headerlink" title="用户行为分析"></a>用户行为分析</h2><h3 id="用户活跃度和物品流行度的分布"><a href="#用户活跃度和物品流行度的分布" class="headerlink" title="用户活跃度和物品流行度的分布"></a>用户活跃度和物品流行度的分布</h3><p>长尾分布：</p>
<script type="math/tex; mode=display">f(x)= \alpha_i k^ {\beta_i}</script><p>物品流行度，用户的活跃度都近似长尾分布</p>
<h3 id="用户活跃度和物品流行度的关系"><a href="#用户活跃度和物品流行度的关系" class="headerlink" title="用户活跃度和物品流行度的关系"></a>用户活跃度和物品流行度的关系</h3><p>一般，用户活跃度低，其浏览物品的流行度高； 反之流行度高</p>
<p>协同过滤算法包括：</p>
<ul>
<li>基于邻域的方法(neighborhood-based)</li>
<li>隐语义模型(latent factor model)</li>
<li>基于图的随机游走算法(random walk on graph)</li>
</ul>
<p>基于邻域的方法包括：</p>
<ul>
<li>基于用户的协同过滤算法</li>
<li>基于物品的协同过滤算法</li>
</ul>
<h2 id="实验设计和算法评测"><a href="#实验设计和算法评测" class="headerlink" title="实验设计和算法评测"></a>实验设计和算法评测</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>MovieLens. 本章着重研究隐反馈数据集中的TopN推荐问题</p>
<h3 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h3><p>将用户行为数据集按照均匀分布随机分成M份，挑选一份作为测试集，将剩下的M-1份作为训练集。进行M次实验，最后取平均值。</p>
<blockquote>
<p>如果数据集够大，模型够简单，为了快速通过离线实验初步地选择算法，也可以只进行一次</p>
</blockquote>
<h3 id="评测指标-1"><a href="#评测指标-1" class="headerlink" title="评测指标"></a>评测指标</h3><p>精度、覆盖率、新颖性</p>
<h2 id="基于邻域的算法"><a href="#基于邻域的算法" class="headerlink" title="基于邻域的算法"></a>基于邻域的算法</h2><h3 id="基于用户的协同过滤"><a href="#基于用户的协同过滤" class="headerlink" title="基于用户的协同过滤"></a>基于用户的协同过滤</h3><h4 id="基础算法"><a href="#基础算法" class="headerlink" title="基础算法"></a>基础算法</h4><p>令N(u)表示用户u曾今有过正反馈的物品集合，令N(v)为用户v曾经有过正反馈的物品集合，u和v的<strong>兴趣相似度</strong></p>
<script type="math/tex; mode=display">w_{uv}= \frac{|N(u) \cap N(v)|}{|N(u) \cup N(v)|}</script><p>或者余弦相似度计算：</p>
<script type="math/tex; mode=display">w_{uv}= \frac{|N(u) \cap N(v)|}{\sqrt{|N(u) || N(v)|}}</script><p>很多用户之间没有相同的交互物品，先计算出有交互物品的用户对，在除以分母$  \sqrt{|N(u)||N(v)|}$。</p>
<p>为此，先建立物品到用户的倒排表，对于每个物品都保存对该物品产生过行为的用户列表。$C[u][v]=K$,说明用户u和v有K个相同物品</p>
<p>物品-用户倒排表：</p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g88f59pltaj309z0a4jrp.jpg" alt="物品-用户.png"></p>
<p>得到相似兴趣度之后，UserCF用一下公式度量用户u对物品i的感兴趣度：</p>
<script type="math/tex; mode=display">p(u,i)=\sum_{v\in S(u,K)\cap N(i)}w_{uv}r_{vi}</script><blockquote>
<p>在MovieLens数据集下的性能显示：<br>准确率和召回率：不和参数K成线性关系，选择合适的K对于获得高的推荐系统精度比较重要<br>流行度：K越大，流行度越大<br>覆盖率：K越大，流行度越大，覆盖率越低</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基于用户余弦相似度的推荐</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">UserCF</span><span class="params">(train, K, N)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算item-&gt;user的倒排</span></span><br><span class="line">    item_user = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> train[user]:</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> item_user:</span><br><span class="line">                item_user[item] = []</span><br><span class="line">            item_user[item].append(user)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算用户相似度矩阵</span></span><br><span class="line">    sim = &#123;&#125;</span><br><span class="line">    num = &#123;&#125;  <span class="comment"># 统计user的交互item数</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> item_user:</span><br><span class="line">        users = item_user[item]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(users)):</span><br><span class="line">            u = users[i]</span><br><span class="line">            <span class="keyword">if</span> u <span class="keyword">not</span> <span class="keyword">in</span> num:</span><br><span class="line">                num[u] = <span class="number">0</span></span><br><span class="line">            num[u] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> u <span class="keyword">not</span> <span class="keyword">in</span> sim:</span><br><span class="line">                sim[u] = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(users)):</span><br><span class="line">                <span class="keyword">if</span> j == i:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                v = users[j]</span><br><span class="line">                <span class="keyword">if</span> v <span class="keyword">not</span> <span class="keyword">in</span> sim[u]:</span><br><span class="line">                    sim[u][v] = <span class="number">0</span></span><br><span class="line">                sim[u][v] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> u <span class="keyword">in</span> sim:</span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> sim[u]:</span><br><span class="line">            sim[u][v] /= math.sqrt(num[u] * num[v])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按照相似度排序</span></span><br><span class="line">    sorted_user_sim = &#123;k: list(sorted(v.items(), \</span><br><span class="line">                                      key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)) \</span><br><span class="line">                       <span class="keyword">for</span> k, v <span class="keyword">in</span> sim.items()&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">#获取接口函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetRecommendation</span><span class="params">(user)</span>:</span></span><br><span class="line">        items = &#123;&#125;</span><br><span class="line">        seen_items = set(train[user])</span><br><span class="line">        <span class="keyword">for</span> v, _ <span class="keyword">in</span> sorted_user_sim[user][:K]:</span><br><span class="line">            <span class="keyword">for</span> item <span class="keyword">in</span> train[v]:</span><br><span class="line">                <span class="comment"># 去掉用户见过的</span></span><br><span class="line">                <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> seen_items:</span><br><span class="line">                    <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> items:</span><br><span class="line">                        items[item] = <span class="number">0</span></span><br><span class="line">                    items[item] += sim[user][v]</span><br><span class="line">        recs = list(sorted(items.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>))[:N]</span><br><span class="line">        <span class="keyword">return</span> recs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> GetRecommendation</span><br></pre></td></tr></table></figure>
<h4 id="用户相似度计算的改进"><a href="#用户相似度计算的改进" class="headerlink" title="用户相似度计算的改进"></a>用户相似度计算的改进</h4><p>新的公式：</p>
<script type="math/tex; mode=display">w_{uv} = \frac{\sum_{i\in N(u)\cap N(v)} \frac{1}{log1+|N(i)|}}{sqrt{|N(u)||N(v)|}}</script><p>该公式通过 $\frac{1}{sqrt{|N(u)||N(v)|}}$ 惩罚了用户u和用户v共同兴趣列表中热门物品对他们相似度的影响</p>
<h4 id="在线使用UserCF的例子"><a href="#在线使用UserCF的例子" class="headerlink" title="在线使用UserCF的例子"></a>在线使用UserCF的例子</h4><p>Digg博客使用UserCF进行博客推荐。增加指标如下：</p>
<ul>
<li>用户反馈增加：用户“顶”和“踩”的行为增加了40%</li>
<li>平均每个用户将从34个具相似兴趣好友那儿获得200条推荐结果</li>
<li>用户和好友的交互活跃度增加了24%</li>
<li>用户评论增加了11%</li>
</ul>
<h3 id="基于物品的协同过滤算法"><a href="#基于物品的协同过滤算法" class="headerlink" title="基于物品的协同过滤算法"></a>基于物品的协同过滤算法</h3><h4 id="基础算法-1"><a href="#基础算法-1" class="headerlink" title="基础算法"></a>基础算法</h4><p>随着网站的用户数目越来越大，计算用户兴趣相似度越来越困难，其运算时间复杂度和空间复杂度的增长和用户数的增长近似平方关系。并且，基于用户的协同过滤很难对推荐结果做出解释</p>
<p>基于物品的协同过滤算法给用户推荐那些和他们之前喜欢的物品相似的物品。ItemCF算法并不利用物品的内容属性计算物品之间的相似度，它主要通过<strong>分析用户的行为记录计算物品之间的相似度</strong></p>
<p>算法:</p>
<ol>
<li>建立用户-物品倒排表（已有），建立两两物品间的用户数矩阵，得到物品之间的余弦相似度矩阵</li>
<li>通过公式计算用户u对一个物品j的兴趣：<script type="math/tex; mode=display">p_{uj} = \sum_{i\in N(u)\cap S(j,K)} w_{ji}r_{ui}</script></li>
</ol>
<p>计算物品相似度的简单例子：<br><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g8bl39uft8j30f40c6gmw.jpg" alt="用户-物品表.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 基于物品余弦相似度的推荐</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ItemCF</span><span class="params">(train, K, N)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    :params: train, 训练数据集</span></span><br><span class="line"><span class="string">    :params: K, 超参数，设置取TopK相似物品数目</span></span><br><span class="line"><span class="string">    :params: N, 超参数，设置取TopN推荐物品数目</span></span><br><span class="line"><span class="string">    :return: GetRecommendation, 推荐接口函数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 计算物品相似度矩阵</span></span><br><span class="line">    sim = &#123;&#125;</span><br><span class="line">    num = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train:</span><br><span class="line">        items = train[user]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(items)):</span><br><span class="line">            u = items[i]</span><br><span class="line">            <span class="keyword">if</span> u <span class="keyword">not</span> <span class="keyword">in</span> num:</span><br><span class="line">                num[u] = <span class="number">0</span></span><br><span class="line">            num[u] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> u <span class="keyword">not</span> <span class="keyword">in</span> sim:</span><br><span class="line">                sim[u] = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(items)):</span><br><span class="line">                <span class="keyword">if</span> j == i: <span class="keyword">continue</span></span><br><span class="line">                v = items[j]</span><br><span class="line">                <span class="keyword">if</span> v <span class="keyword">not</span> <span class="keyword">in</span> sim[u]:</span><br><span class="line">                    sim[u][v] = <span class="number">0</span></span><br><span class="line">                sim[u][v] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> u <span class="keyword">in</span> sim:</span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> sim[u]:</span><br><span class="line">            sim[u][v] /= math.sqrt(num[u] * num[v])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按照相似度排序</span></span><br><span class="line">    sorted_item_sim = &#123;k: list(sorted(v.items(), \</span><br><span class="line">                                      key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)) \</span><br><span class="line">                       <span class="keyword">for</span> k, v <span class="keyword">in</span> sim.items()&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取接口函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetRecommendation</span><span class="params">(user)</span>:</span></span><br><span class="line">        items = &#123;&#125;</span><br><span class="line">        seen_items = set(train[user])</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> train[user]:</span><br><span class="line">            <span class="keyword">for</span> u, _ <span class="keyword">in</span> sorted_item_sim[item][:K]:</span><br><span class="line">                <span class="keyword">if</span> u <span class="keyword">not</span> <span class="keyword">in</span> seen_items:</span><br><span class="line">                    <span class="keyword">if</span> u <span class="keyword">not</span> <span class="keyword">in</span> items:</span><br><span class="line">                        items[u] = <span class="number">0</span></span><br><span class="line">                    items[u] += sim[item][u]</span><br><span class="line">        recs = list(sorted(items.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>))[:N]</span><br><span class="line">        <span class="keyword">return</span> recs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> GetRecommendation</span><br></pre></td></tr></table></figure>
<h4 id="用户活跃度对物品相似度的影响"><a href="#用户活跃度对物品相似度的影响" class="headerlink" title="用户活跃度对物品相似度的影响"></a>用户活跃度对物品相似度的影响</h4><blockquote>
<p>IUF: 活跃用户对物品相似度的贡献应该小于不活跃的用户。</p>
</blockquote>
<p>修正物品相似度的计算公式：</p>
<script type="math/tex; mode=display">w_{ij} = \frac{\sum _{u\in N(i)\cap N(j)} \frac{1}{log1+|N(u)|}}{\sqrt{|N(i)||N(j)|}}</script><h4 id="物品相似度的归一化"><a href="#物品相似度的归一化" class="headerlink" title="物品相似度的归一化"></a>物品相似度的归一化</h4><p>将ItemCFde的相似矩阵按最大值归一化，可以提高推荐的准确率，如果已经得到物品相似度矩阵w,使用如下公式得到w’:</p>
<script type="math/tex; mode=display">w'_{ij} = \frac{w_{ij}}{\max _j  w_{ij}}</script><blockquote>
<p>归一化不仅增加推荐的准确度，它还可以提高推荐的覆盖率和多样性</p>
</blockquote>
<h4 id="实验结果："><a href="#实验结果：" class="headerlink" title="实验结果："></a>实验结果：</h4><p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g8ckbzxa25j30b206daa6.jpg" alt="Item结果.png"></p>
<p>Item-IUF 提高了准确率和召回率，但覆盖率、流行度指标有所下降<br>Item-Norm所有指标都有所提升。</p>
<h3 id="UserCF与ItemCF的比较"><a href="#UserCF与ItemCF的比较" class="headerlink" title="UserCF与ItemCF的比较"></a>UserCF与ItemCF的比较</h3><p>Digg使用UserCF,亚马逊使用ItemCF。<br>UserCF的推荐更社会化，反映了用户所在的小型兴趣群体中物品的热门程度，而ItemCF的推荐更加个性化，反映了用户自己的兴趣传承</p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g8ckh6xi3kj30lj0b7tc0.jpg" alt="UserCF&amp; ItemCF优缺点比较.png"></p>
<blockquote>
<p>在离线实验中，最<strong>原始</strong>的UserCF和ItemCF中，往往ItemCF各项指标都不如UserCF.</p>
</blockquote>
<h4 id="哈利波特问题"><a href="#哈利波特问题" class="headerlink" title="哈利波特问题"></a>哈利波特问题</h4><p>加大对物品的惩罚：</p>
<script type="math/tex; mode=display">w_{ij} = \frac{|N(i)\cap N(j)|}{|N(i)|^{1-\alpha} |N(j)^\alpha |}</script><p>其中α∈[0.5,1]。通过提高α，可以惩罚热门的j。<br>从离线实验中，α只有在取值为0.5时才会导致最高的准确率和召回率。α越大，覆盖率越高，平均结果的平均人们会降低。可以通过这种方法适当牺牲准确率和召回率来提高覆盖率和新颖性。</p>
<blockquote>
<p>两个不同领域的最热门物品之间往往具有比较高的相似度。这时，仅仅依靠用户行为数据是不能解决的。只能依靠引入物品的内容数据解决这个问题。比如对不同领域的物品降低权重</p>
</blockquote>
<h2 id="隐语义模型-latent-factor-model"><a href="#隐语义模型-latent-factor-model" class="headerlink" title="隐语义模型(latent factor model)"></a>隐语义模型(latent factor model)</h2><p>隐含语义分析技术(latent variable analysis)采取基于用户行为统计的自动聚类。</p>
<p>可以解决如下问题：</p>
<ul>
<li>编辑的意见不能代表用户的意见</li>
<li>编辑很难控制分类的粒度</li>
<li>编辑很难给一个物品多个分类</li>
<li>编辑很难给出多维度的分类</li>
<li>编辑很难决定一个物品在某一个分类中的权重</li>
</ul>
<p>LFM计算用户u对物品i的兴趣：</p>
<script type="math/tex; mode=display">Preference(u,i) =r_{ui} =p_{u} ^T q_i    =\sum_{f=1} ^F p_{u,k}q_{i,k}</script><p>公式中 $p_{u,k}$ 和 $q_{i,k}$是模型的参数，其中$p_{u,k}$度量了用户u的兴趣和第k个隐类的关系，$q_{i,k}$度量了第k个隐类和物品i之间的关系。</p>
<p><strong>算法主要流程</strong>：  </p>
<ul>
<li>采样</li>
<li>损失函数</li>
<li>梯度下降</li>
<li>随机梯度下降</li>
</ul>
<p>由于隐形反馈数据集没有负样本，因此首先要<strong>采样</strong>: 这里选取那些很热门，而用户却没有行为的物品</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 负采样函数(注意！！！要按照流行度进行采样)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">nSample</span><span class="params">(data, ratio)</span>:</span></span><br><span class="line">        new_data = &#123;&#125;</span><br><span class="line">        <span class="comment"># 正样本</span></span><br><span class="line">        <span class="keyword">for</span> user <span class="keyword">in</span> data:</span><br><span class="line">            <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> new_data:</span><br><span class="line">                new_data[user] = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> item <span class="keyword">in</span> data[user]:</span><br><span class="line">                new_data[user][item] = <span class="number">1</span></span><br><span class="line">        <span class="comment"># 负样本</span></span><br><span class="line">        <span class="keyword">for</span> user <span class="keyword">in</span> new_data:</span><br><span class="line">            seen = set(new_data[user])</span><br><span class="line">            pos_num = len(seen)</span><br><span class="line">            item = np.random.choice(items, int(pos_num * ratio * <span class="number">3</span>), p=pops)  <span class="comment"># 按照pops的概率进行选择</span></span><br><span class="line">            item = [x <span class="keyword">for</span> x <span class="keyword">in</span> item <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> seen][:int(pos_num * ratio)]</span><br><span class="line">            new_data[user].update(&#123;x: <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> item&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> new_data</span><br></pre></td></tr></table></figure>
<p>采样后，得到一个用户-物品集K={(u,i)},其中如果(u,i)是正样本，则有$r_{ui}$=1,否则有$r_{ui}$=0。利用如下<strong>损失函数</strong>来找最合适的参数p和q</p>
<script type="math/tex; mode=display">C=\sum _{(u,i)\in K}(r_{ui}- \widehat{r}_{ui})^2=\sum_{(u,i)\in K}(r_{ui} - \sum_{k=1}^K p_{u,k}q_{i,k})^2 + \lambda\parallel p_u\parallel ^2+ \lambda\parallel q_i\parallel ^2</script><p>这里，$\lambda\parallel p_u\parallel ^2+ \lambda\parallel q_i\parallel ^2$ 是用来防止过拟合的正则化项。λ可通过实验获得。</p>
<p><strong>梯度公式</strong></p>
<script type="math/tex; mode=display">\frac{\partial C}{\partial p_{uk}}=-2(r_{ui} - \sum_{k=1}^K p_{u,k}q_{i,k})q_{ik} +2\lambda p_{uk}</script><script type="math/tex; mode=display">\frac{\partial C}{\partial q_{ik}}=-2(r_{ui} - \sum_{k=1}^K p_{u,k}q_{i,k})p_{uk} +2\lambda q_{ik}</script><p><strong>随机梯度下降</strong></p>
<script type="math/tex; mode=display">p_{uk}=p_{uk}+\alpha ((r_{ui} - \sum_{k=1}^K p_{u,k}q_{i,k})q_{ik} - \lambda p_{uk}</script><script type="math/tex; mode=display">q_{ik}=q_{ik}+\alpha ((r_{ui} - \sum_{k=1}^K p_{u,k}q_{i,k})p_{uk} - \lambda q_{ik}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line">   P, Q = &#123;&#125;, &#123;&#125;</span><br><span class="line">   <span class="keyword">for</span> user <span class="keyword">in</span> train:</span><br><span class="line">       P[user] = np.random.random(K)</span><br><span class="line">   <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">       Q[item] = np.random.random(K)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">for</span> s <span class="keyword">in</span> trange(step):</span><br><span class="line">       data = nSample(train, ratio)</span><br><span class="line">       <span class="keyword">for</span> user <span class="keyword">in</span> data:</span><br><span class="line">           <span class="keyword">for</span> item <span class="keyword">in</span> data[user]:</span><br><span class="line">               eui = data[user][item] - (P[user] * Q[item]).sum()</span><br><span class="line">               <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, K):</span><br><span class="line">                   P[user] += lr * (Q[item] * eui - lmbda * P[user])</span><br><span class="line">                   Q[item] += lr * (P[user] * eui - lmbda * Q[item])</span><br><span class="line">       lr *= <span class="number">0.9</span>  <span class="comment"># 调整学习率</span></span><br><span class="line"></span><br><span class="line">   <span class="comment"># 获取接口函数</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">GetRecommendation</span><span class="params">(user)</span>:</span></span><br><span class="line">       seen_items = set(train[user])</span><br><span class="line">       recs = &#123;&#125;</span><br><span class="line">       <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">           <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> seen_items:</span><br><span class="line">               recs[item] = (P[user] * Q[item])</span><br><span class="line">       recs = list(sorted(recs.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>))[:N]</span><br><span class="line">       <span class="keyword">return</span> recs</span><br><span class="line"></span><br><span class="line">   <span class="keyword">return</span> GetRecommendation</span><br></pre></td></tr></table></figure>
<p><strong>结果</strong></p>
<p>选取4个隐类中排名最高（$q_{ik}$最大）的一些电影，结果表明，每一类电影都是合理的，都代表了一类用户喜欢看的电影。</p>
<h3 id="基于LFM的实际系统例子"><a href="#基于LFM的实际系统例子" class="headerlink" title="基于LFM的实际系统例子"></a>基于LFM的实际系统例子</h3><p>雅虎新闻推荐:</p>
<p>LFM训练十分耗时，一般在实际应用中只能每天训练一次。雅虎使用如下公式预测用户u是否会单机链接i</p>
<script type="math/tex; mode=display">r_{ui}=x_u ^T \cdot y_i + p_u ^T \cdot q_i</script><p>其中$y_i$是根据物品的内容属性直接生成的，$x_{uk}$是用户u对内容特征k的兴趣程度。用户向量$x_k$可以根据历史行为记录获得，每天只计算一些。而$p_u$、$q_i$是根据实时拿到的用户最近几小时的行为训练LFM获得的。</p>
<h3 id="LFM和基于邻域的方法的比较"><a href="#LFM和基于邻域的方法的比较" class="headerlink" title="LFM和基于邻域的方法的比较"></a>LFM和基于邻域的方法的比较</h3><ul>
<li><p>理论基础： LFM是一种基于机器学习的方法，邻域是一种统计学方法，没有学习过程</p>
</li>
<li><p>离线计算的空间复杂度： 假设有M个用户和N个物品。 用户相关表需要O(M×M)的空间。物品相关表需要O(N×N)的空间。 LFM需要O(F×(M+N))。LFM大量节省了训练过程中的内存</p>
</li>
<li><p>离线计算的时间复杂度： 假设有M个用户，N个物品，K条用户对物品的行为记录。 UserCF的时间复杂度 O（N×(K/N)^2);  ItemCF的时间复杂度 O(M×(K/M)^2); 如果有K个隐类，迭代S次，LFM的时间复杂度 O(F×S×K)。 总体上没有质的差别</p>
</li>
<li><p>在线实时推荐： ItemCF一旦用户有了新行为，推荐列表马上发生变化。 对于LFM,当物品量很多时，O(M×N×F)，因此LFM不适合物品数非常庞大的系统。所以，当用户有了新行为，LFM的推荐列表不会发生变化</p>
</li>
<li><p>推荐解释： ItemCF可以支持很好的解释。LFM无法提供解释</p>
</li>
</ul>
<h2 id="基于图的模型"><a href="#基于图的模型" class="headerlink" title="基于图的模型"></a>基于图的模型</h2><h3 id="用户行为数据的二分图表示（graph-based-model"><a href="#用户行为数据的二分图表示（graph-based-model" class="headerlink" title="用户行为数据的二分图表示（graph-based model)"></a>用户行为数据的二分图表示（graph-based model)</h3><p>令G(V,E)表示用户物品二分图：</p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g8d3cy0wb2j30f606cdg9.jpg" alt="用户物品二分图.png"></p>
<h3 id="基于图的推荐算法"><a href="#基于图的推荐算法" class="headerlink" title="基于图的推荐算法"></a>基于图的推荐算法</h3><p>图中顶点的相关度主要取决于以下因素：</p>
<ul>
<li>两个顶点之间路径数</li>
<li>两个顶点之间路径长度</li>
<li>两个顶点之间路径经过的顶点</li>
</ul>
<p>相关度高的顶点一般有如下特性：</p>
<ul>
<li>两个顶点有很多路径相连</li>
<li>连接两个顶点之间的路径长度比较短</li>
<li>连接两个顶点之间的路径不会经过出度较大的顶点</li>
</ul>
<h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><p>从用户u对应的结点$v_u$开始，在任何一个节点，以α的概率往下走，1-α的概率回到$v_u$节点。如果继续游走，则从当前节点指向的节点中按照均匀分布随机选择一个节点作为游走下次经过的结点。经过很多次随机游走之后，每个物品结点被访问到的概率会收敛到一个数。</p>
<p>公式如下：</p>
<script type="math/tex; mode=display">PR(v)=\begin{cases}\alpha \sum_{v'\in in(v)} \frac{PR(v')}{|out(v')|} & (v\neq v_u)\\(1-\alpha)+\alpha\sum_{v'\in in(v)} \frac{PR(v')}{|out(v')|} & (v=v_u)\end{cases}</script><p>公式中PR(i)表示物品i的访问概率(物品i的权重),out(i)表示物品节点i的出度。</p>
<h5 id="算法举例"><a href="#算法举例" class="headerlink" title="算法举例"></a>算法举例</h5><p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g8d4kdep6yj307707n74p.jpg" alt="二分图举例.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PersonalRank</span><span class="params">(G, alpha, root, max_depth)</span>:</span></span><br><span class="line">    rank = dict()</span><br><span class="line">    rank = &#123;x: <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> G.keys()&#125;</span><br><span class="line">    rank[root] = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 开始迭代</span></span><br><span class="line">    begin = time.time()</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(max_depth):</span><br><span class="line">        tmp = &#123;x: <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> G.keys()&#125;</span><br><span class="line">        <span class="comment"># 取出节点i和他的出边尾节点集合ri</span></span><br><span class="line">        <span class="keyword">for</span> i, ri <span class="keyword">in</span> G.items():</span><br><span class="line">            <span class="comment"># 取节点i的出边的尾节点j以及边E(i,j)的权重wij,边的权重都为1，归一化后就是1/len(ri)</span></span><br><span class="line">            <span class="keyword">for</span> j, wij <span class="keyword">in</span> ri.items():</span><br><span class="line">                <span class="keyword">if</span> j <span class="keyword">not</span> <span class="keyword">in</span> tmp:</span><br><span class="line">                    tmp[j] = <span class="number">0</span></span><br><span class="line">                tmp[j] += alpha * rank[i] / (<span class="number">1.0</span> * len(ri))</span><br><span class="line">                <span class="keyword">if</span> j == root:</span><br><span class="line">                    tmp[root] += (<span class="number">1</span> - alpha)</span><br><span class="line">        rank = tmp</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">'use_time'</span>, end-begin)</span><br><span class="line">    lst = sorted(rank.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> ele <span class="keyword">in</span> lst:</span><br><span class="line">        print(<span class="string">"%s:%.3f, \t"</span> % (ele[<span class="number">0</span>], ele[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> rank</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    alpha = <span class="number">0.6</span></span><br><span class="line">    G = &#123;<span class="string">'A'</span>: &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'c'</span>: <span class="number">1</span>&#125;,</span><br><span class="line">         <span class="string">'B'</span>: &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">1</span>, <span class="string">'c'</span>: <span class="number">1</span>, <span class="string">'d'</span>: <span class="number">1</span>&#125;,</span><br><span class="line">         <span class="string">'C'</span>: &#123;<span class="string">'c'</span>: <span class="number">1</span>, <span class="string">'d'</span>: <span class="number">1</span>&#125;,</span><br><span class="line">         <span class="string">'a'</span>: &#123;<span class="string">'A'</span>: <span class="number">1</span>, <span class="string">'B'</span>: <span class="number">1</span>&#125;,</span><br><span class="line">         <span class="string">'b'</span>: &#123;<span class="string">'B'</span>: <span class="number">1</span>&#125;,</span><br><span class="line">         <span class="string">'c'</span>: &#123;<span class="string">'A'</span>: <span class="number">1</span>, <span class="string">'B'</span>: <span class="number">1</span>, <span class="string">'C'</span>: <span class="number">1</span>&#125;,</span><br><span class="line">         <span class="string">'d'</span>: &#123;<span class="string">'B'</span>: <span class="number">1</span>, <span class="string">'C'</span>: <span class="number">1</span>&#125;&#125;</span><br><span class="line">    PersonalRank(G, alpha, <span class="string">'A'</span>, <span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p>结果得到对于A,d的访问概率大于b。<br>PersonalRank算法在时间复杂度上有明显的缺点。对每个用户进行推荐时，都需要在整个用户物品二分图上进行迭代，直到整个图上的每个顶点的PR值收敛。</p>
<h4 id="矩阵形式PersonalRank"><a href="#矩阵形式PersonalRank" class="headerlink" title="矩阵形式PersonalRank"></a>矩阵形式PersonalRank</h4><p>令M为用户物品二分图的转移概率矩阵</p>
<script type="math/tex; mode=display">M_{ij}=\begin{cases}\frac{1}{|out(j)|} & if(j\in out(i))\\0 & else\end{cases}</script><p>迭代公式变为：</p>
<script type="math/tex; mode=display">r=(1-\alpha)r_0+\alpha M^T r</script><p>其中，r是个n维向量，$r_0$代表起点，第i个位置上是1，其余元素均为0。解出方程，得到：</p>
<script type="math/tex; mode=display">r=(1-\alpha)(1-\alpha M_T)^{-1}r_0</script><p>只需计算一次 $(1-\alpha M_T)^{-1}$,快速求解。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from numpy.linalg import solve</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    alpha = 0.8</span><br><span class="line">    vertex = [&apos;A&apos;, &apos;B&apos;, &apos;C&apos;, &apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;]</span><br><span class="line">    M = np.matrix([[0,        0,        0,        0.5,      0,        0.5,      0],</span><br><span class="line">                   [0,        0,        0,        0.25,     0.25,     0.25,     0.25],</span><br><span class="line">                   [0,        0,        0,        0,        0,        0.5,      0.5],</span><br><span class="line">                   [0.5,      0.5,      0,        0,        0,        0,        0],</span><br><span class="line">                   [0,        1.0,      0,        0,        0,        0,        0],</span><br><span class="line">                   [0.333,    0.333,    0.333,    0,        0,        0,        0],</span><br><span class="line">                   [0,        0.5,      0.5,      0,        0,        0,        0]])</span><br><span class="line">    r0 = np.matrix([[1], [0], [0], [0], [0], [0], [0]])  # 从&apos;A&apos;开始游走</span><br><span class="line">    print(r0.shape)</span><br><span class="line">    n = M.shape[0]</span><br><span class="line">    # 直接解线性方程法</span><br><span class="line">    A = np.eye(n)-alpha*M.T</span><br><span class="line">    b = (1-alpha)*r0</span><br><span class="line">    begin = time.time()</span><br><span class="line">    r = solve(A, b)</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(&apos;user time&apos;, end-begin)</span><br><span class="line">    print(r)</span><br><span class="line">    rank = &#123;&#125;</span><br><span class="line">    for j in range(n):</span><br><span class="line">        rank[vertex[j]] = r[j][0]</span><br><span class="line">    li = sorted(rank.items(), key=lambda x: x[1], reverse=True)</span><br><span class="line">    for ele in li:</span><br><span class="line">        print(&quot;%s:%.3f,\t&quot; % (ele[0], ele[1]))</span><br></pre></td></tr></table></figure>
<p>这里采用直接解线性方程法，还可以对求解进行优化，采用CSC矩阵压缩等方法。</p>
<h1 id="基于标签的推荐"><a href="#基于标签的推荐" class="headerlink" title="基于标签的推荐"></a>基于标签的推荐</h1><p><img src="http://ww1.sinaimg.cn/large/006M8Ovlgy1gaxf6w6fwpj30k1068t8w.jpg" alt="联系途径.png"></p>
<p>标签一般分为两种：</p>
<ul>
<li>作者或专家给物品打标签</li>
<li>让普通用户打标签， 也就是UGC（User Generated Content）</li>
</ul>
<h1 id="推荐系统实例"><a href="#推荐系统实例" class="headerlink" title="推荐系统实例"></a>推荐系统实例</h1><h2 id="外围架构"><a href="#外围架构" class="headerlink" title="外围架构"></a>外围架构</h2><p><img src="http://ww1.sinaimg.cn/large/006M8Ovlgy1gaxizpxt0vj30ge04qwem.jpg" alt="外部架构.png"></p>
<p><strong>数据收集和存储</strong></p>
<p>从<em>实时存储</em>的角度看，购买、收藏、评论、评分等行为都需要实时存取。而浏览网页的行为和搜索行为并不需要实时存取。</p>
<p>从<em>存储媒介</em>上来看。需要实时存取的数据存储在数据库和缓存中，大规模的非实时地存取数据存储在分布式文件系统（HDFS)中。</p>
<h2 id="推荐系统架构（基于特征）"><a href="#推荐系统架构（基于特征）" class="headerlink" title="推荐系统架构（基于特征）"></a>推荐系统架构（基于特征）</h2><p><strong>特征种类</strong></p>
<ul>
<li>人口统计学特征  </li>
</ul>
<p>包括用户的年龄、性别、国籍信息</p>
<ul>
<li>用户的行为特征</li>
<li>用户的话题特征</li>
</ul>
<p><strong>推荐系统架构图</strong></p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlgy1gaxjfyiqx5j30n406bq3h.jpg" alt="推荐系统的架构图.png"></p>
<p>推荐系统需要由多个推荐引擎组成，<strong>每个推荐引擎负责一类特征和一种任务</strong>。推荐系统的任务是将推荐引擎的结果按照一定权重或者优先级合并、排序然后返回。</p>
<h2 id="推荐引擎的架构"><a href="#推荐引擎的架构" class="headerlink" title="推荐引擎的架构"></a>推荐引擎的架构</h2><p><img src="http://ww1.sinaimg.cn/large/006M8Ovlgy1gaxjkixypsj30it0c1759.jpg" alt="推荐引擎架构图.png"></p>
<h3 id="生成用户特征向量"><a href="#生成用户特征向量" class="headerlink" title="生成用户特征向量"></a>生成用户特征向量</h3><p>计算特征向量时考虑一下因素：</p>
<ul>
<li>用户行为的种类</li>
<li>用户行为产生的时间</li>
<li>用户行为的次数</li>
<li>物品的热门程度</li>
</ul>
<h3 id="特征-物品相关推荐"><a href="#特征-物品相关推荐" class="headerlink" title="特征-物品相关推荐"></a>特征-物品相关推荐</h3><p>对于每个特征，可以在<strong>相关表</strong>中存储和它最相关的N个物品的ID</p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlgy1gaxjvbz1rfj30w002v74a.jpg" alt="相关表.png"></p>
<p>特征-物品相关推荐模块可以接受一个候选物品集合。目的是保证推荐结果只包含候选物品集合中的物品。</p>
<h3 id="过滤模块"><a href="#过滤模块" class="headerlink" title="过滤模块"></a>过滤模块</h3><p>过滤以下物品：</p>
<ul>
<li>用户已经产生过行为的物品</li>
<li>候选物品以外的物品</li>
<li>某些质量很差的物品</li>
</ul>
<h3 id="排名模块"><a href="#排名模块" class="headerlink" title="排名模块"></a>排名模块</h3><ul>
<li>新颖性排名</li>
<li>多样性</li>
<li>时间多样性</li>
<li>用户反馈</li>
</ul>
<p>排名模块最重要的部分就是用户反馈模块。用户反馈模块主要通过分析用户之前和推荐结果的交互日志，预测用户会对什么样的推荐结果比较感兴趣。 eg： 点击率预测</p>

          
        
      
    </div>
    
    
    

    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/30/hello world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kason">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/westbrook.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kason's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/30/hello world/" itemprop="url">Hexo</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-30T11:33:34+08:00">
                2019-09-30
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/09/30/hello world/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/09/30/hello world/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Hexo基本操作"><a href="#Hexo基本操作" class="headerlink" title="Hexo基本操作"></a>Hexo基本操作</h2><h3 id="本地预览"><a href="#本地预览" class="headerlink" title="本地预览"></a>本地预览</h3><p><code>hexo s --debug</code></p>
<h3 id="生成部署"><a href="#生成部署" class="headerlink" title="生成部署"></a>生成部署</h3><p><code>hexo g</code></p>
<p><code>hexo d</code></p>
<hr>
<h2 id="Markdown基本操作"><a href="#Markdown基本操作" class="headerlink" title="Markdown基本操作"></a>Markdown基本操作</h2><h3 id="多行代码"><a href="#多行代码" class="headerlink" title="多行代码"></a>多行代码</h3><ol>
<li>使用三个反引号</li>
<li>在每一行前面缩进4个空格</li>
</ol>
<p>```<strong>key</strong></p>
<p>代码块</p>
<p>```</p>
<p>key可以为cpp,java,python，key进行<strong>高亮</strong></p>
<h3 id="单行代码"><a href="#单行代码" class="headerlink" title="单行代码"></a>单行代码</h3><p>使用反引号 ` 来标记或插入代码区段</p>
<p>eg：打印使用`代码`来进行输出</p>

          
        
      
    </div>
    
    
    

    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/westbrook.jpg" alt="Kason">
            
              <p class="site-author-name" itemprop="name">Kason</p>
              <p class="site-description motion-element" itemprop="description">Kason's technology blog</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Kasonreal" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:shiyuxiang99@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://tristone13th.github.io/" title="云中君" target="_blank">云中君</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://101.201.69.42/" title="木偶" target="_blank">木偶</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kason</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'R4rICLTQsNXFhLpVLOKUtwY8-MdYXbMMI',
        appKey: 'a3s2kYOiN3ocHpSSUvnOw2BM',
        placeholder: '来说两句吧',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  
  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
