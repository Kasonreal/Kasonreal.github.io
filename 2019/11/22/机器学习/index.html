<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="机器学习引言监督学习(Supervised learning)数据集中的每个样本都有相应的“正确答案”，根据这些样本作出预测。 监督学习的类型 回归： 推出一个连续的输出         分类： 推出一组离散的结果  非监督学习(Unsupervised learning)在未加标签的数据中，试图找到隐藏的结构。 非监督学习的类型聚类，降维，隐马尔可夫模型等。 聚类的应用： 谷歌新闻：将非常多的新">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习">
<meta property="og:url" content="http://yoursite.com/2019/11/22/机器学习/index.html">
<meta property="og:site_name" content="Kason&#39;s Blog">
<meta property="og:description" content="机器学习引言监督学习(Supervised learning)数据集中的每个样本都有相应的“正确答案”，根据这些样本作出预测。 监督学习的类型 回归： 推出一个连续的输出         分类： 推出一组离散的结果  非监督学习(Unsupervised learning)在未加标签的数据中，试图找到隐藏的结构。 非监督学习的类型聚类，降维，隐马尔可夫模型等。 聚类的应用： 谷歌新闻：将非常多的新">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006M8Ovlly1g7qq9p01v9j309f07jweb.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006M8Ovlly1g9ah3vzgxdj30ef05lq39.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006M8Ovlly1g9ahes3t3aj30jv05wt9q.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006M8Ovlly1g9alf9j5qwj31360qvq4o.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006M8Ovlly1g9ahes3t3aj30jv05wt9q.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006M8Ovlly1g9g8gscmvaj30he0bn76e.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006M8Ovlly1g9g8rzzas0j30fo0apjsn.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006M8Ovlly1g9g8zh54ubj30fv09vmxw.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006M8Ovlly1g9g8zsog2dj30db09374y.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006M8Ovlly1g9hdw1lzgnj30f30ad0tt.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006M8Ovlly1g9hdyurbzcj30fk0ajgmy.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006M8Ovlly1g9he0grto0j30l30blwf8.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006M8Ovlly1g9hfk2oj4pj30fu06874o.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006M8Ovlly1g9hfy4wretj30cs09a0tw.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006M8Ovlly1g9ie3fs8a6j30ei05k3yv.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006M8Ovlly1g9ieg65ga3j30ag04jglo.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006M8Ovlly1g9ii4rukprj309f060q3a.jpg">
<meta property="og:updated_time" content="2019-12-02T09:23:20.583Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习">
<meta name="twitter:description" content="机器学习引言监督学习(Supervised learning)数据集中的每个样本都有相应的“正确答案”，根据这些样本作出预测。 监督学习的类型 回归： 推出一个连续的输出         分类： 推出一组离散的结果  非监督学习(Unsupervised learning)在未加标签的数据中，试图找到隐藏的结构。 非监督学习的类型聚类，降维，隐马尔可夫模型等。 聚类的应用： 谷歌新闻：将非常多的新">
<meta name="twitter:image" content="http://ww1.sinaimg.cn/large/006M8Ovlly1g7qq9p01v9j309f07jweb.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/11/22/机器学习/">





  <title>机器学习 | Kason's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Kason's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/22/机器学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kason">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/westbrook.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kason's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-22T10:41:03+08:00">
                2019-11-22
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/11/22/机器学习/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/11/22/机器学习/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="机器学习引言"><a href="#机器学习引言" class="headerlink" title="机器学习引言"></a>机器学习引言</h1><h2 id="监督学习-Supervised-learning"><a href="#监督学习-Supervised-learning" class="headerlink" title="监督学习(Supervised learning)"></a>监督学习(Supervised learning)</h2><p>数据集中的每个样本都有相应的“正确答案”，根据这些样本作出<em>预测</em>。</p>
<h3 id="监督学习的类型"><a href="#监督学习的类型" class="headerlink" title="监督学习的类型"></a>监督学习的类型</h3><ul>
<li>回归： 推出一个<strong>连续</strong>的输出        </li>
<li>分类： 推出一组<strong>离散</strong>的结果</li>
</ul>
<h2 id="非监督学习-Unsupervised-learning"><a href="#非监督学习-Unsupervised-learning" class="headerlink" title="非监督学习(Unsupervised learning)"></a>非监督学习(Unsupervised learning)</h2><p>在未加标签的数据中，试图找到隐藏的结构。</p>
<h3 id="非监督学习的类型"><a href="#非监督学习的类型" class="headerlink" title="非监督学习的类型"></a>非监督学习的类型</h3><p><em>聚类</em>，<em>降维</em>，<em>隐马尔可夫模型</em>等。</p>
<h3 id="聚类的应用："><a href="#聚类的应用：" class="headerlink" title="聚类的应用："></a><strong>聚类</strong>的应用：</h3><ol>
<li>谷歌新闻：将非常多的新闻事件<em>自动地</em>聚类到一起  </li>
<li>基因学的应用：根据基因将个体聚类到不同的类或组  </li>
<li>社交网络的分析：根据Facebook或email自动给出朋友的分组  </li>
<li>市场分析  </li>
</ol>
<h1 id="线性回归-Linear-Regression"><a href="#线性回归-Linear-Regression" class="headerlink" title="线性回归(Linear Regression)"></a>线性回归(Linear Regression)</h1><h2 id="描述回归问题的标记"><a href="#描述回归问题的标记" class="headerlink" title="描述回归问题的标记"></a>描述回归问题的标记</h2><p>$m$ 代表训练集中实例的数量</p>
<p>$x$  代表特征/输入变量</p>
<p>$y$ 代表目标变量/输出变量</p>
<p>$\left( x,y \right)$ 代表训练集中的实例</p>
<p>$\left(x^\left(i\right),y^\left(i\right)\right)$ 代表第$i$ 个观察实例</p>
<p>$h$  代表学习算法的解决方案或函数也称为假设（<strong>hypothesis</strong>）</p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g7qq9p01v9j309f07jweb.jpg" alt="回归流程.png"></p>
<p><strong>h的表达形式</strong>：</p>
<p>一种表达形式 $h_\theta \left( x \right)=\theta_{0} + \theta_{1}x_1 +···+\theta_nx_n$</p>
<p><strong>代价函数：</strong>（均方误差）</p>
<p>$J(\theta_0,\theta_1,···\theta_n)=\frac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2$</p>
<p>这里除以m是希望之后计算梯度时大小不随样本数量的增多而增大</p>
<p><strong>批量梯度下降算法：</strong></p>
<p>$\theta_j:=\theta_j-\alpha\frac{\partial }{\partial \theta_j}J(\theta_0,\theta_1,···\theta_n)  $    </p>
<p>线性回归问题可以改写为：</p>
<p>$\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)}·x_0^{(i)})$               $x_0^{(i)}=1$</p>
<p>$\theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)}·x_1^{(i)})$ </p>
<p>$···$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">theta = np.zeros(X.shape[<span class="number">1</span>]) <span class="comment"># theta代表特征（变量）数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta, X, y)</span>:</span>  <span class="comment"># 求导，这里将x0赋为1，同其他变量一起求导</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]  <span class="comment"># 样本数</span></span><br><span class="line">    inner = X.T @ (X @ theta - y)</span><br><span class="line">    <span class="keyword">return</span> inner / m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_gradient_decent</span><span class="params">(theta, X, y, epoch, alpha=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">    _theta = theta.copy()</span><br><span class="line">    cost_data = [lr_cost(theta, X, y)]</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(epoch):</span><br><span class="line">        _theta = _theta - alpha * gradient(_theta, X, y)</span><br><span class="line">        cost_data.append(lr_cost(_theta, X, y))</span><br><span class="line">    <span class="keyword">return</span> _theta, cost_data</span><br></pre></td></tr></table></figure>
<h2 id="梯度下降实践"><a href="#梯度下降实践" class="headerlink" title="梯度下降实践"></a>梯度下降实践</h2><p><strong>特征缩放</strong></p>
<p>最简单的方法:   $x_n=\frac{x_n-\mu_n}{s_n}$，其中$\mu_n$是平均值，$s_n$是范围</p>
<p><strong>学习率</strong></p>
<p>画出代价函数随迭代次数的图像。若出现上升情形，很可能是因为学习率$\alpha$取值过大</p>
<p>通常考虑$\alpha=0.03,0.3,1,3,10$</p>
<h2 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h2><p>算术求解出最优$\theta$</p>
<p>$\theta=(X^TX)^{-1}X^Ty$</p>
<p>大多数情况下，$(X^TX)^{-1}$是可逆的，若不可逆，有以下<strong>两种情况</strong>：</p>
<ul>
<li>特征值里有一些多余的特征，如果其中$x_1$和$x_2$是线性相关的，会导致不可逆，我们需要删除重复特征里的其中一个</li>
<li>特征数量太多(m&lt;=n)，导致不可逆。可以用较少的特征来反映尽可能多内容，或者考虑使用正则化的方法。</li>
</ul>
<p><strong>$\theta$的推导过程（略）</strong></p>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>在分类问题中，逻辑回归（Logistic Regression)是目前最流行使用最广泛的一种学习算法。</p>
<h2 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h2><p><strong>模型假设</strong></p>
<p>$h_\theta(x)=g(\theta^TX)$  其中 X代表特征向量，g代表逻辑函数</p>
<p>一个常用的逻辑函数为S形函数，公式为$g(z)=\frac{1}{1+e^{-z}}$</p>
<p><strong>代价函数</strong>（交叉熵）</p>
<p>若仍使用平方误差，得到的代价函数是非凸函数，因此采用交叉熵</p>
<p>$J(\theta ) = -\frac{1}{m}\left[\sum_{i=1}^{m}y^{(i)}log(h_\theta (x^{(i)}))+(1-y^{(i)})log(1-h_\theta (x^{(i)})) \right]$</p>
<p>其中，$h_\theta (x^{(i)}) = \frac{1}{1+e^{-\theta^\mathrm {T} x}}$</p>
<p><strong>梯度下降</strong></p>
<p>$\frac{\partial J(\theta)}{\partial \theta_{j}} = \frac{1}{m}\left[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_{j}^{(i)}\right]$</p>
<p>$\theta_j:=\theta_j-\alpha\frac{1}{m}\left[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_{j}^{(i)}\right]$</p>
<p>即使更新参数的规则看起来和线性回归一致，由于假设的定义发生了变化，所以是完全不同的</p>
<p><strong>高级优化</strong></p>
<p>除了梯度下降法，我们还可以采用其他方法进行优化，比如共轭梯度法，BFGS（变尺度法），L-BFGS（限制变尺度法）。这三种算法不需要手动选择学习率$\alpha$，它们内部有一个<strong>线性搜索</strong>算法</p>
<h2 id="一对多"><a href="#一对多" class="headerlink" title="一对多"></a>一对多</h2><p>使用二分类的思想，将一个类别标记为正向类，其他的所有类标记为负向类，进行多次分类。</p>
<p>得到的一系列模型记为： $h_\theta^{(i)}(x)=p(y=i|x;\theta)$其中：i=(1,2,…k)</p>
<p><strong>目标</strong></p>
<p>$\max_i h_\theta^{(i)}(x)$</p>
<p>在n个分类器中输入x，选择一个让$h_\theta^{(i)}(x)$最大的i</p>
<h2 id="Exercise-多项式特征映射"><a href="#Exercise-多项式特征映射" class="headerlink" title="Exercise(多项式特征映射)"></a>Exercise(多项式特征映射)</h2><p>如果样本量多，逻辑问题很复杂<strong>原始特征</strong>只有x1，x2，可以用多项式创建更多的特征。因为更多的特征可以得到的分割线可以是<strong>高阶</strong>函数的形状</p>
<p>eg: 有a,b两个特征，那么它的2次多项式的次数为[1,a,b,$a^2$,ab,$b^2$]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_mapping</span><span class="params">(x, y, power, as_ndarray=False)</span>:</span></span><br><span class="line"><span class="comment">#     """return mapped features as ndarray or dataframe"""</span></span><br><span class="line">    <span class="comment"># data = &#123;&#125;</span></span><br><span class="line">    <span class="comment"># # inclusive</span></span><br><span class="line">    <span class="comment"># for i in np.arange(power + 1):</span></span><br><span class="line">    <span class="comment">#     for p in np.arange(i + 1):</span></span><br><span class="line">    <span class="comment">#         data["f&#123;&#125;&#123;&#125;".format(i - p, p)] = np.power(x, i - p) * np.power(y, p)</span></span><br><span class="line"></span><br><span class="line">    data = &#123;<span class="string">"f&#123;&#125;&#123;&#125;"</span>.format(i - p, p): np.power(x, i - p) * np.power(y, p)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(power + <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> np.arange(i + <span class="number">1</span>)</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure>
<p>其中power的值代表最高阶次数</p>
<h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><ol>
<li>逻辑回归为什么是线性的？而神经网络不是线性的？</li>
</ol>
<p>主要依据<strong>决策边界</strong>。LR的决策边界是线性的。$\hat\theta·x=0$是线性的</p>
<p>而神经网络的边界不是线性的。</p>
<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p><strong>如何解决过拟合：</strong></p>
<ul>
<li>减少特征变量</li>
<li>正则化。保留所有的特征，但是减少参数的大小  </li>
</ul>
<h2 id="正则化线性回归"><a href="#正则化线性回归" class="headerlink" title="正则化线性回归"></a>正则化线性回归</h2><p><strong>正则化线性回归的代价函数：</strong></p>
<p>$J( \theta)=\frac{1}{2m}[ {\sum\limits_{i = 1}^m ( h_\theta( x^{(i)}  - y^{(i)}) ^2+ \lambda \sum\limits_{j = 1}^n {\theta _j^2} } ]$</p>
<p>此时梯度下降法：</p>
<p>Repeat{</p>
<p>${\theta _0}: = {\theta _0} - \alpha \left[ {\frac{1}{m}\sum\limits_{i = 1}^m {\left( {h_\theta \left( {x^{\left( i \right)}} \right) - {y^{\left( i \right)}}} \right)x_0^{\left( i \right)}} } \right]$</p>
<script type="math/tex; mode=display">{\theta _j}: = {\theta _j} - \alpha \left[ {\frac{1}{m}\sum\limits_{i = 1}^m {\left( {h_\theta \left( {x^{\left( i \right)}} \right) - {y^{\left( i \right)}}} \right)x_j^{\left( i \right)} + \frac{\lambda }{m}{\theta _j}} } \right]\left( {j = 1,2,...,n} \right)</script><p>}</p>
<p><strong>正规方程求解：</strong></p>
<script type="math/tex; mode=display">\theta = {\left( {X^TX + \lambda \left[ {\begin{array}{*{20}{c}} 0&0&0&0\\ 0&1&0&0\\ .&.&.&.\\ 0&0&0&1 \end{array}} \right]} \right)^{ - 1}}{X^T}y</script><p>图中的矩阵尺寸为（n+1)*（n+1)</p>
<h2 id="正则化逻辑回归"><a href="#正则化逻辑回归" class="headerlink" title="正则化逻辑回归"></a>正则化逻辑回归</h2><p> <strong>正规化逻辑回归代价函数</strong></p>
<p>$J(\theta ) = -\frac{1}{m}\left[\sum_{i=1}^{m}y^{(i)}log(h_\theta (x^{(i)}))+(1-y^{(i)})log(1-h_\theta (x^{(i)})) \right]+\frac{\lambda}{2m}\sum_\limits{j=1}^n\theta_j^2$</p>
<p>此时梯度下降法：</p>
<p>Repeat{</p>
<p>${\theta _0}: = {\theta _0} - \alpha \left[ {\frac{1}{m}\sum\limits_{i = 1}^m {\left( {h_\theta \left( {x^{\left( i \right)}} \right) - {y^{\left( i \right)}}} \right)x_0^{\left( i \right)}} } \right]$</p>
<p>${\theta _j}: = {\theta _j} - \alpha \left[ {\frac{1}{m}\sum\limits_{i = 1}^m {\left( {h_\theta \left( {x^{\left( i \right)}} \right) - {y^{\left( i \right)}}} \right)x_j^{\left( i \right)} + \frac{\lambda }{m}{\theta _j}} } \right]\left( {j = 1,2,…,n} \right)$</p>
<p>}</p>
<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><p><strong>代价函数</strong></p>
<p>$J(\Theta) = -\frac{ 1 }{ m }[\sum_\limits{ i=1 }^{ m } \sum_\limits{ k=1 }^{ k } ({y_k^{(i)} \log(h_\Theta(x^{(i)}))_k + (1 - y_k^{(i)}) \log (1 - (h_\Theta(x^{(i)}))_k})]+\frac{\lambda}{2m}\sum_\limits{l=1}^{L-1}\sum_\limits{i=1}^{s_l}\sum_\limits{j=1}^{s_{l+1}}(\Theta_{ji}^{(l)})^2)$</p>
<p>其中 ${\left( {h_\Theta }\left( x \right) \right)_i}$ = 神经网络的第i个输出</p>
<p>正则化项中：最里层的j循环所有的行，i循环所有的列，最外层是神经元的层数。</p>
<h2 id="向前传播"><a href="#向前传播" class="headerlink" title="向前传播"></a>向前传播</h2><p>公式$a_j^l=\sigma(\sum\limits_kw_{jk}^l\alpha_k^{l-1}+b_j^l)$</p>
<p>$b_j^l$表示在$l^{th}$层第$j^{th}$个神经元的偏置，$\alpha_j^{l}$表示$l^{th}$层第$j^{th}$个神经元的激活值</p>
<p>$w_{jk}^l$是从$(l-1)^{th}$层的第$k^{th}$个神经元到$l^{th}$层的第$j^{th}$个神经元的连接上的权重</p>
<p>对于一个四层的神经网络</p>
<p>$\begin{array}{l} {a^{\left( 1 \right)}} = x\\ {z^{\left( 2 \right)}} = {\Theta ^{\left( 1 \right)}}{a^{\left( 1 \right)}}\\ {a^{\left( 2 \right)}} = g\left( {z^{\left( 2 \right)}} \right)\left( { + a_0^{\left( 2 \right)}} \right)\\ {z^{\left( 3 \right)}} = {\Theta ^{\left( 2 \right)}}{a^{\left( 2 \right)}}\\ {a^{\left( 3 \right)}} = g\left( {z^{\left( 3 \right)}} \right)\left( { + a_0^{\left( 3 \right)}} \right)\\ {z^{\left( 4 \right)}} = {\Theta ^{\left( 3 \right)}}{a^{\left( 3 \right)}}\\ {a^{\left( 4 \right)}} = {h_\Theta }\left( x \right) = g\left( {z^{\left( 4 \right)}} \right) \end{array}$</p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>反向传播的目标是计算代价函数C分别关于w和b的偏导数$\frac{\partial C}{\partial w}$和$\frac{\partial C}{\partial b}$</p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9ah3vzgxdj30ef05lq39.jpg" alt="反向传播1.png"></p>
<p><strong>反向传播算法描述</strong></p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9ahes3t3aj30jv05wt9q.jpg" alt="反向传播2.png"></p>
<p><strong>梯度检查</strong></p>
<p>$\frac{\text{d}}{\text{d}\Theta}J(\Theta)\approx\frac{J(\Theta+\epsilon)-J(\Theta-\epsilon)}{2\epsilon}$</p>
<p>没有使用这种方法计算偏导数的原因：反向传播可以同时计算所有的偏导数$\frac{\partial C}{\partial w}$</p>
<p><strong>随机初始化</strong></p>
<p>初始化所有的$\theta$为0是不行的，因此随机初始化参数矩阵</p>
<h2 id="Exercise（手写数字识别）"><a href="#Exercise（手写数字识别）" class="headerlink" title="Exercise（手写数字识别）"></a>Exercise（手写数字识别）</h2><h3 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h3><ul>
<li>一维模型（一对多分类）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">    y_matrix.append((raw_y == k).astype(int))</span><br></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9alf9j5qwj31360qvq4o.jpg" alt="向量化标签.png"></p>
<p>采用LR，一次只能在2个类之间进行分类</p>
<ul>
<li>k维模型</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">k_theta = np.array([logistic_regression(X, y[k]) <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">10</span>)])</span><br><span class="line">prob_matrix = sigmoid(X @ k_theta.T)</span><br><span class="line">y_pred = np.argmax(prob_matrix, axis=<span class="number">1</span>) <span class="comment">#返回沿轴axis最大值的索引，axis=1代表行</span></span><br><span class="line">y_answer = raw_y.copy()</span><br><span class="line">y_answer[y_answer==<span class="number">10</span>] = <span class="number">0</span>  <span class="comment"># 之前已将标签为10移至第1列</span></span><br><span class="line">print(classification_report(y_answer, y_pred))</span><br></pre></td></tr></table></figure>
<p>可以进行所有标签预测</p>
<ul>
<li>前馈预测（默认已有weight和b矩阵）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_weight</span><span class="params">(path)</span>:</span></span><br><span class="line">    data = sio.loadmat(path)</span><br><span class="line">    <span class="keyword">return</span> data[<span class="string">'Theta1'</span>], data[<span class="string">'Theta2'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(path)</span>:</span></span><br><span class="line">    data = sio.loadmat(path)</span><br><span class="line">    y = data.get(<span class="string">'y'</span>)  <span class="comment"># (5000,1)</span></span><br><span class="line">    y = y.reshape(y.shape[<span class="number">0</span>])  <span class="comment"># make it back to column vector</span></span><br><span class="line"></span><br><span class="line">    X = data.get(<span class="string">'X'</span>)  <span class="comment"># (5000,400)</span></span><br><span class="line">    <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">theta1, theta2 = load_weight(<span class="string">'ex3weights.mat'</span>)</span><br><span class="line"></span><br><span class="line">X, y = load_data(<span class="string">'ex3data1.mat'</span>)</span><br><span class="line"></span><br><span class="line">X = np.insert(X, <span class="number">0</span>, values=np.ones(X.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)  <span class="comment"># intercept</span></span><br><span class="line">a1 = X</span><br><span class="line">z2 = a1 @ theta1.T  <span class="comment"># (5000, 401) @ (25,401).T = (5000, 25)</span></span><br><span class="line">z2 = np.insert(z2, <span class="number">0</span>, values=np.ones(z2.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)</span><br><span class="line">a2 = sigmoid(z2)</span><br><span class="line">z3 = a2 @ theta2.T</span><br><span class="line">a3 = sigmoid(z3)</span><br><span class="line">y_pred = np.argmax(a3, axis=<span class="number">1</span>) + <span class="number">1</span>   <span class="comment"># numpy is 0 base index, +1 for matlab convention</span></span><br><span class="line"></span><br><span class="line">print(classification_report(y, y_pred))</span><br></pre></td></tr></table></figure>
<h3 id="反向传播-1"><a href="#反向传播-1" class="headerlink" title="反向传播"></a>反向传播</h3><p>首先进行<strong>正向传播</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_forward</span><span class="params">(theta, X)</span>:</span></span><br><span class="line">    <span class="string">"""apply to architecture 400+1 * 25+1 *10</span></span><br><span class="line"><span class="string">    X: 5000 * 401</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    t1, t2 = deserialize(theta)  <span class="comment"># t1: (25,401) t2: (10,26)</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    a1 = X  <span class="comment"># 5000 * 401</span></span><br><span class="line"></span><br><span class="line">    z2 = a1 @ t1.T  <span class="comment"># 5000 * 25</span></span><br><span class="line">    a2 = np.insert(sigmoid(z2), <span class="number">0</span>, np.ones(m), axis=<span class="number">1</span>)  <span class="comment"># 5000*26</span></span><br><span class="line"></span><br><span class="line">    z3 = a2 @ t2.T  <span class="comment"># 5000 * 10</span></span><br><span class="line">    h = sigmoid(z3)  <span class="comment"># 5000*10, this is h_theta(X)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a1, z2, a2, z3, h  <span class="comment"># you need all those for backprop</span></span><br></pre></td></tr></table></figure>
<p>接着进行<strong>反向梯度下降</strong></p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9ahes3t3aj30jv05wt9q.jpg" alt="反向传播2.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 反向传播梯度下降</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    <span class="comment"># initialize</span></span><br><span class="line">    t1, t2 = deserialize(theta)  <span class="comment"># t1: (25,401) t2: (10,26)</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    delta1 = np.zeros(t1.shape)  <span class="comment"># (25, 401)</span></span><br><span class="line">    delta2 = np.zeros(t2.shape)  <span class="comment"># (10, 26)</span></span><br><span class="line"></span><br><span class="line">    a1, z2, a2, z3, h = feed_forward(theta, X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        a1i = a1[i, :]  <span class="comment"># (1, 401)</span></span><br><span class="line">        z2i = z2[i, :]  <span class="comment"># (1, 25)</span></span><br><span class="line">        a2i = a2[i, :]  <span class="comment"># (1, 26)</span></span><br><span class="line"></span><br><span class="line">        hi = h[i, :]    <span class="comment"># (1, 10)</span></span><br><span class="line">        yi = y[i, :]    <span class="comment"># (1, 10)</span></span><br><span class="line"></span><br><span class="line">        d3i = hi - yi  <span class="comment"># (1, 10)    </span></span><br><span class="line"></span><br><span class="line">        z2i = np.insert(z2i, <span class="number">0</span>, np.ones(<span class="number">1</span>))  <span class="comment"># make it (1, 26) to compute d2i</span></span><br><span class="line">        d2i = np.multiply(t2.T @ d3i, sigmoid_gradient(z2i))  <span class="comment"># (1, 26)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># careful with np vector transpose</span></span><br><span class="line">        delta2 += np.matrix(d3i).T @ np.matrix(a2i)  <span class="comment"># (1, 10).T @ (1, 26) -&gt; (10, 26)</span></span><br><span class="line">        delta1 += np.matrix(d2i[<span class="number">1</span>:]).T @ np.matrix(a1i)  <span class="comment"># (1, 25).T @ (1, 401) -&gt; (25, 401)</span></span><br><span class="line"></span><br><span class="line">    delta1 = delta1 / m</span><br><span class="line">    delta2 = delta2 / m</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> serialize(delta1, delta2)</span><br></pre></td></tr></table></figure>
<p><strong>注：</strong>$d3i$即为输出层误差，$d2i$为隐含层误差</p>
<p>这里返回的delta1，delta2是C对w求偏导的结果，这里的w包括b</p>
<h3 id="梯度校验"><a href="#梯度校验" class="headerlink" title="梯度校验"></a>梯度校验</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_checking</span><span class="params">(theta, X, y, epsilon, regularized=False)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">a_numeric_grad</span><span class="params">(plus, minus, regularized=False)</span>:</span></span><br><span class="line">        <span class="string">"""calculate a partial gradient with respect to 1 theta"""</span></span><br><span class="line">        <span class="keyword">if</span> regularized:</span><br><span class="line">            <span class="keyword">return</span> (regularized_cost(plus, X, y) - regularized_cost(minus, X, y)) / (epsilon * <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (cost(plus, X, y) - cost(minus, X, y)) / (epsilon * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    theta_matrix = expand_array(theta)  <span class="comment"># expand to (10285, 10285)</span></span><br><span class="line">    epsilon_matrix = np.identity(len(theta)) * epsilon</span><br><span class="line"></span><br><span class="line">    plus_matrix = theta_matrix + epsilon_matrix</span><br><span class="line">    minus_matrix = theta_matrix - epsilon_matrix</span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculate numerical gradient with respect to all theta</span></span><br><span class="line">    numeric_grad = np.array([a_numeric_grad(plus_matrix[i], minus_matrix[i], regularized)</span><br><span class="line">                                    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(theta))])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># analytical grad will depend on if you want it to be regularized or not</span></span><br><span class="line">    analytic_grad = regularized_gradient(theta, X, y) <span class="keyword">if</span> regularized <span class="keyword">else</span> gradient(theta, X, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If you have a correct implementation, and assuming you used EPSILON = 0.0001</span></span><br><span class="line">    <span class="comment"># the diff below should be less than 1e-9</span></span><br><span class="line">    <span class="comment"># this is how original matlab code do gradient checking</span></span><br><span class="line">    diff = np.linalg.norm(numeric_grad - analytic_grad) / np.linalg.norm(numeric_grad + analytic_grad)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">expand_array</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="string">"""replicate array into matrix</span></span><br><span class="line"><span class="string">    [1, 2, 3]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    [[1, 2, 3],</span></span><br><span class="line"><span class="string">     [1, 2, 3],</span></span><br><span class="line"><span class="string">     [1, 2, 3]]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># turn matrix back to ndarray</span></span><br><span class="line">    <span class="keyword">return</span> np.array(np.matrix(np.ones(arr.shape[<span class="number">0</span>])).T @ np.matrix(arr))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行</span></span><br><span class="line">gradient_checking(theta, X, y, epsilon= <span class="number">0.0001</span>)<span class="comment">#这个运行很慢，谨慎运行</span></span><br></pre></td></tr></table></figure>
<p>If your backpropagation implementation is correct，the relative difference will be smaller than 10e-9 (assume epsilon=0.0001)</p>
<h1 id="机器学习诊断法"><a href="#机器学习诊断法" class="headerlink" title="机器学习诊断法"></a>机器学习诊断法</h1><h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><p>一般使用60%的数据作为训练集，使用20%的数据作为<strong>交叉验证集</strong>，20%的数据作为测试集。</p>
<p>这样比只分训练集和测试集相比，可以得到更好的泛化误差。</p>
<p><strong>模型选择的方法</strong></p>
<ol>
<li>使用训练集训练出（比如）10个模型</li>
<li>用这些模型在交叉验证集中计算交叉验证误差（代价函数）</li>
<li>选取交叉验证误差最小的模型</li>
<li>利用这个模型计算测试集中的泛化误差（代价函数的值）</li>
</ol>
<p>训练误差：</p>
<p>$J_{train}(\Theta)=\frac{1}{2m}\sum_\limits{i=1}^m (h_\Theta(x^{(i)})-y^{(i)})^2$</p>
<p>交叉验证误差：</p>
<p>$J_{CV}(\Theta)=\frac{1}{2m}\sum_\limits{i=1}^m (h_\Theta(x_{CV}^{(i)})-y_{CV}^{(i)})^2$</p>
<h2 id="偏差-Bias-和方差-Variance"><a href="#偏差-Bias-和方差-Variance" class="headerlink" title="偏差(Bias)和方差(Variance)"></a>偏差(Bias)和方差(Variance)</h2><p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9g8gscmvaj30he0bn76e.jpg" alt="方差和偏差.png"></p>
<p>如何判断是欠拟合（偏差）还是过拟合（方差）：</p>
<ul>
<li><p>训练集误差和交叉验证集误差近似时：偏差/欠拟合- </p>
</li>
<li><p>交叉验证集误差&gt;&gt;训练集误差时：方差/过拟合</p>
</li>
</ul>
<h2 id="正则化-1"><a href="#正则化-1" class="headerlink" title="正则化"></a>正则化</h2><p>对于${h_\theta }\left( x \right) = {\theta _0} + {\theta _1}x + {\theta _2}{x^2} + {\theta _3}{x^3} + {\theta _4}{x^4}$</p>
<p>及$J( \theta)=\frac{1}{2m}[ {\sum\limits_{i = 1}^m ( h_\theta( x^{(i)}  - y^{(i)}) ^2+ \lambda \sum\limits_{j = 1}^n {\theta _j^2} } ]$</p>
<p><strong>选择$\lambda$的方法</strong></p>
<p>类似选择模型的方法，将训练集和交叉验证集的代价函数误差与$\lambda$的值绘制在一张图表上</p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9g8rzzas0j30fo0apjsn.jpg" alt="正则化偏差和方差.png"></p>
<ul>
<li>当$\lambda$较小时，训练集误差较小（过拟合），交叉验证集误差较大</li>
<li>随着$\lambda$的增加，训练集误差不断增加（欠拟合），而交叉验证集误差先减小后增加</li>
</ul>
<h2 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h2><p>学习曲线是学习算法的一个很好的合理检验（sanity check）。绘制训练集误差和交叉验证集误差随训练实例数量（m）的变化图</p>
<p><strong>高偏差</strong></p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9g8zh54ubj30fv09vmxw.jpg" alt="高偏差.png"></p>
<p><strong>高方差</strong></p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9g8zsog2dj30db09374y.jpg" alt="高方差.png"></p>
<ul>
<li>对于欠拟合，增加数据到训练集不一定有帮助</li>
<li>对于过拟合，增加更多数据到训练集可能提高算法效果</li>
</ul>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><strong>改进算法策略</strong></p>
<ol>
<li>获取更多的训练实例   —解决高方差</li>
<li>尝试减少特征的数量  —解决高方差</li>
<li>获取更多特征  —解决高偏差</li>
<li>增加多项式特征  —解决高偏差</li>
<li>减少正则化程度$\lambda$  —解决高偏差</li>
<li>增加正则化程度$\lambda$  —解决高方差</li>
</ol>
<h3 id="神经网络中的偏差和方差"><a href="#神经网络中的偏差和方差" class="headerlink" title="神经网络中的偏差和方差"></a>神经网络中的偏差和方差</h3><ul>
<li>使用较小的神经网络，容易导致高偏差和欠拟合</li>
<li>使用较大地神经网络，容易导致高方差和过拟合</li>
</ul>
<p>通常选择较大的神经网络并采用正则化处理，这样比使用较小网络效果要好</p>
<p>选择隐含层的方法和选择模型的方法类似。</p>
<h1 id="机器学习系统设计"><a href="#机器学习系统设计" class="headerlink" title="机器学习系统设计"></a>机器学习系统设计</h1><h2 id="学习算法的推荐方法"><a href="#学习算法的推荐方法" class="headerlink" title="学习算法的推荐方法"></a><strong>学习算法的推荐方法</strong></h2><ol>
<li>简答快速的实现一个算法，并用交叉验证集数据测试这个算法</li>
<li>绘制学习曲线，决定是增加更多数据，特征还是其他</li>
<li>进行<strong>误差分析</strong>：人工检查交叉验证集中我们算法产生预测误差的实例，看看这些实例是否有某种系统化变化的趋势</li>
</ol>
<h2 id="类偏斜"><a href="#类偏斜" class="headerlink" title="类偏斜"></a>类偏斜</h2><p>类偏斜（skewed classes）：训练集中有非常多的同一种类的实例，只有很少或没有其他类的实例</p>
<p><strong>分类问题指标</strong></p>
<ul>
<li>真阳性 TP : 预测为正，实际为正</li>
<li>假阳性 FP：预测为正，实际为负</li>
<li>假阴性 FN：预测为负，实际为正</li>
<li>真阴性 TN：预测为负，实际为负</li>
</ul>
<p><strong>查准率</strong>：预测为正的样本中有多少正样本</p>
<p>$P=\frac{TP}{TP+FP}$</p>
<p><strong>查全率：</strong>样本中的正例有多少被预测正确了</p>
<p>$R=\frac{TP}{TP+FN}$</p>
<p><strong>F1-Score</strong></p>
<p>衡量二分类模型精确度的一种指标</p>
<p>$F_1=2·\frac{precision·recall}{precision+recall}$</p>
<h2 id="机器学习数据"><a href="#机器学习数据" class="headerlink" title="机器学习数据"></a>机器学习数据</h2><p>得到一个性能很好的学习算法思路：</p>
<ul>
<li>特征值有足够多信息，这样可以保证低偏差</li>
</ul>
<h1 id="支持向量机（SVM"><a href="#支持向量机（SVM" class="headerlink" title="支持向量机（SVM)"></a>支持向量机（SVM)</h1><h2 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h2><p>在逻辑回归中，对于一个样本（x,y)来说，它的损失函数为：$-ylog(\frac{1}{1+e^{-\theta^Tx}})-(1-y)log(1-\frac{1}{1+e^{-\theta^Tx}})$</p>
<ul>
<li>对于y=1的情况，损失函数变为$-log(\frac{1}{1+e^{-z}})$</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9hdw1lzgnj30f30ad0tt.jpg" alt="y=1.png"></p>
<p>支持向量机将绿线替换成红线，称改变后的损失函数为：$Cost_1(z)$</p>
<ul>
<li>对于y=0的情况</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9hdyurbzcj30fk0ajgmy.jpg" alt="y=0.png"></p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9he0grto0j30l30blwf8.jpg" alt="svm假设函数.png"></p>
<p>因此，支持向量机的目标是：</p>
<p>$\underbrace{min}_\theta\left\{C\left[\sum_\limits{i=1}^my^{(i)}Cost_1\left(\theta^Tx^{(i)}\right)+\left(1-y^{(i)}\right)Cost_0\left(\theta^Tx^{(i)}\right)\right]+\frac{1}{2}\sum_\limits{j=1}^n\theta_j^2\right\}$</p>
<p>其中，假设函数为：</p>
<p>$h_\theta(x)=\begin{cases}1 &amp; \theta^Tx\geq0\\0 &amp; \theta^Tx&lt;0\end{cases}$</p>
<h2 id="大间距分类器（Large-Margin-Classifier"><a href="#大间距分类器（Large-Margin-Classifier" class="headerlink" title="大间距分类器（Large Margin Classifier)"></a>大间距分类器（Large Margin Classifier)</h2><p>支持向量机的<strong>目标</strong>是：</p>
<p>$\underbrace{min}_\theta\left\{C\left[\sum_\limits{i=1}^my^{(i)}Cost_1\left(\theta^Tx^{(i)}\right)+\left(1-y^{(i)}\right)Cost_0\left(\theta^Tx^{(i)}\right)\right]+\frac{1}{2}\sum_\limits{j=1}^n\theta_j^2\right\}$</p>
<p>下图分别是$Cost_1(z)$和$Cost_0(z)$的示意图：</p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9hfk2oj4pj30fu06874o.jpg" alt="svm cost.png"></p>
<p>如果y=1，我们想要$\theta^Tx\geq1$（而不仅仅是$\theta^Tx\geq0)$</p>
<p>如果y=0，我们想要$\theta^Tx\leq-1$（而不仅仅是$\theta^Tx&lt;0)$</p>
<p><strong>参数C对decision boundary的影响</strong></p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9hfy4wretj30cs09a0tw.jpg" alt="svm边界.png"></p>
<p>当C非常大时，会得到黄色的线，此时过拟合</p>
<p>当C不是适当时，它可以忽略掉一些异常点的影响，得到更好的决策界</p>
<p><strong>大间距分类器的原因</strong></p>
<p>对于代价目标的后半部分：$\frac{1}{2}\sum_\limits{j=1}^n\theta_j^2$=$\frac{1}{2}\parallel\theta\parallel^2$</p>
<p>当y=1时，$\theta^Tx$会朝着$\theta^Tx\geq1$的趋势去优化$\theta$，加上第二部分的$\theta$尽量小，则对于</p>
<p>$\theta^Tx^{(i)}=p^{(i)}\parallel\theta\parallel$，$p^{(i)}$需要尽量大</p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9ie3fs8a6j30ei05k3yv.jpg" alt="大间距分类器.png"></p>
<p>支持向量机会选择后者，因为它有较大的p</p>
<h2 id="内核"><a href="#内核" class="headerlink" title="内核"></a>内核</h2><p><strong>高斯内核</strong></p>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9ieg65ga3j30ag04jglo.jpg" alt="高斯内核.png"></p>
<p>这里的”similarity”函数就是“内核”，这种内核又称为“<strong>高斯内核</strong>”</p>
<p>当x距离$l^{(1)}$越近，$f_1$越接近于1，越远越接近于0</p>
<p><strong>选定l</strong></p>
<p>通常根据训练集的数量选择地标的数量，即如果训练集中有m个实例，则选取m个地标。并且令：$l^{(1)}=x^{(1)}$，…，$l^{(m)}=x^{(m)}$。现在支持向量机的任务是：</p>
<p>$\underbrace{min}_\theta\left\{C\left[\sum_\limits{i=1}^my^{(i)}Cost_1\left(\theta^Tf^{(i)}\right)+\left(1-y^{(i)}\right)Cost_0\left(\theta^Tf^{(i)}\right)\right]+\frac{1}{2}\sum_\limits{j=1}^n\theta_j^2\right\}$</p>
<p>并且对正则化做出调整，计算$\sum_\limits{j=1}^{n=m}\theta^2=\theta^T\theta$时，用$\theta^TM\theta$代替$\theta^T\theta$，其中M是根据我们选择的核函数而不同的一个矩阵，这样做简化了计算。</p>
<p><strong>参数影响</strong></p>
<ul>
<li>C较大，过拟合，高方差</li>
<li>C较小，欠拟合，高偏差</li>
<li>$\sigma$较大，低方差，高偏差</li>
<li>$\sigma$较小，低偏差，高方差</li>
</ul>
<h2 id="SVM使用"><a href="#SVM使用" class="headerlink" title="SVM使用"></a>SVM使用</h2><p><strong>准则</strong></p>
<p>n为特征数，m为训练样本数。</p>
<ul>
<li>n比m大很多，选用逻辑回归模型或者线性支持向量机</li>
<li>n较小，m大小中等，比如n在1-1000之间，m在10-10000之间，使用高斯内核的向量机</li>
<li>n较小，m较大，n在1-1000之间，m&gt;50000，使用支持向量机会很慢，解决方案：创造增加更多的特征，然后使用逻辑回归或者线性内核</li>
<li>一般，支持向量机比神经网络快，因为SVM的代价函数是凸函数，不存在局部最小值</li>
</ul>
<h2 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h2><h3 id="线性内核"><a href="#线性内核" class="headerlink" title="线性内核"></a>线性内核</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> sklearn.svm</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mat = sio.loadmat(<span class="string">'./data/ex6data1.mat'</span>)</span><br><span class="line">data = pd.DataFrame(mat.get(<span class="string">'X'</span>), columns=[<span class="string">'X1'</span>, <span class="string">'X2'</span>])</span><br><span class="line">data[<span class="string">'y'</span>] = mat.get(<span class="string">'y'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># try C=1</span></span><br><span class="line">svc1 = sklearn.svm.LinearSVC(C=<span class="number">1</span>, loss=<span class="string">'hinge'</span>)</span><br><span class="line">svc1.fit(data[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]], data[<span class="string">'y'</span>])</span><br><span class="line"><span class="comment"># 类别预测的置信水平，这是该点与超平面距离的函数。</span></span><br><span class="line">data[<span class="string">'SVM1 Confidence'</span>] = svc1.decision_function(data[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]])</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">ax.scatter(data[<span class="string">'X1'</span>], data[<span class="string">'X2'</span>], s=<span class="number">50</span>, c=data[<span class="string">'SVM1 Confidence'</span>], cmap=<span class="string">'RdBu'</span>)</span><br><span class="line">ax.set_title(<span class="string">'SVM (C=1) Decision Confidence'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># try C=100</span></span><br><span class="line"><span class="comment"># 此时过拟合，decision_function的值变大</span></span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：svc.decision_function含义，C的取值影响</p>
<h3 id="Gaussion-kernels"><a href="#Gaussion-kernels" class="headerlink" title="Gaussion kernels"></a>Gaussion kernels</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"></span><br><span class="line"><span class="comment"># kernek function 高斯核函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian_kernel</span><span class="params">(x1, x2, sigma)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.exp(- np.power(x1 - x2, <span class="number">2</span>).sum() / (<span class="number">2</span> * (sigma ** <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mat = sio.loadmat(<span class="string">'./data/ex6data2.mat'</span>)</span><br><span class="line">data = pd.DataFrame(mat.get(<span class="string">'X'</span>), columns=[<span class="string">'X1'</span>, <span class="string">'X2'</span>])</span><br><span class="line">data[<span class="string">'y'</span>] = mat.get(<span class="string">'y'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># matplotlib画出分类图</span></span><br><span class="line"><span class="comment"># positive = data[data['y'].isin([1])]</span></span><br><span class="line"><span class="comment"># negative = data[data['y'].isin([0])]</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># fig, ax = plt.subplots(figsize=(12,8))</span></span><br><span class="line"><span class="comment"># ax.scatter(positive['X1'], positive['X2'], s=30, marker='x', label='Positive')</span></span><br><span class="line"><span class="comment"># ax.scatter(negative['X1'], negative['X2'], s=30, marker='o', label='Negative')</span></span><br><span class="line"><span class="comment"># ax.legend()</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将使用内置的RBF内核构建支持向量机分类器，并检查其对训练数据的准确性</span></span><br><span class="line">svc = svm.SVC(C=<span class="number">100</span>, kernel=<span class="string">'rbf'</span>, gamma=<span class="number">10</span>, probability=<span class="literal">True</span>)</span><br><span class="line">svc.fit(data[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]], data[<span class="string">'y'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># the predict_proba显示类别，是positive还是negative</span></span><br><span class="line">predict_prob = svc.predict_proba(data[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]])[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">ax.scatter(data[<span class="string">'X1'</span>], data[<span class="string">'X2'</span>], s=<span class="number">30</span>, c=predict_prob, cmap=<span class="string">'Reds'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>注意：画图方法，svc.predict_proba含义</p>
<h3 id="选择参数"><a href="#选择参数" class="headerlink" title="选择参数"></a>选择参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"></span><br><span class="line">mat = sio.loadmat(<span class="string">'./data/ex6data3.mat'</span>)</span><br><span class="line">training = pd.DataFrame(mat.get(<span class="string">'X'</span>), columns=[<span class="string">'X1'</span>, <span class="string">'X2'</span>])</span><br><span class="line">training[<span class="string">'y'</span>] = mat.get(<span class="string">'y'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉验证集</span></span><br><span class="line">cv = pd.DataFrame(mat.get(<span class="string">'Xval'</span>), columns=[<span class="string">'X1'</span>, <span class="string">'X2'</span>])</span><br><span class="line">cv[<span class="string">'y'</span>] = mat.get(<span class="string">'yval'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># manual grid search</span></span><br><span class="line">candidate = [<span class="number">0.01</span>, <span class="number">0.03</span>, <span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">30</span>, <span class="number">100</span>]</span><br><span class="line">combination = [(C, gamma) <span class="keyword">for</span> C <span class="keyword">in</span> candidate <span class="keyword">for</span> gamma <span class="keyword">in</span> candidate]</span><br><span class="line">search = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> C, gamma <span class="keyword">in</span> combination:</span><br><span class="line">    svc = svm.SVC(C=C, gamma=gamma)</span><br><span class="line">    svc.fit(training[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]], training[<span class="string">'y'</span>])</span><br><span class="line">    search.append(svc.score(cv[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]], cv[<span class="string">'y'</span>]))</span><br><span class="line"></span><br><span class="line">best_score = search[np.argmax(search)]</span><br><span class="line">best_param = combination[np.argmax(search)]</span><br><span class="line"></span><br><span class="line">best_svc = svm.SVC(C=<span class="number">100</span>, gamma=<span class="number">0.3</span>)</span><br><span class="line">best_svc.fit(training[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]], training[<span class="string">'y'</span>])</span><br><span class="line">ypred = best_svc.predict(cv[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]])</span><br><span class="line"></span><br><span class="line">print(metrics.classification_report(cv[<span class="string">'y'</span>], ypred))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># sklearn GridSearchCV</span></span><br><span class="line">parameters = &#123;<span class="string">'C'</span>: candidate, <span class="string">'gamma'</span>: candidate&#125;</span><br><span class="line">svc = svm.SVC()</span><br><span class="line">clf = GridSearchCV(svc, parameters, n_jobs=<span class="number">-1</span>)</span><br><span class="line">clf.fit(training[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]], training[<span class="string">'y'</span>])</span><br><span class="line">ypred = clf.predict(cv[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]])</span><br><span class="line">print(metrics.classification_report(cv[<span class="string">'y'</span>], ypred))</span><br></pre></td></tr></table></figure>
<p> 自动搜索与人工搜索搜索得到<strong>参数不同</strong>的原因:<br> 自动搜索只使用了部分training data,把剩余training当做CV.</p>
<h3 id="垃圾邮件分类"><a href="#垃圾邮件分类" class="headerlink" title="垃圾邮件分类"></a>垃圾邮件分类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"></span><br><span class="line">mat_tr = sio.loadmat(<span class="string">'data/spamTrain.mat'</span>)</span><br><span class="line">X, y = mat_tr.get(<span class="string">'X'</span>), mat_tr.get(<span class="string">'y'</span>).ravel()</span><br><span class="line"></span><br><span class="line">mat_test = sio.loadmat(<span class="string">'data/spamTest.mat'</span>)</span><br><span class="line">test_X, test_y = mat_test.get(<span class="string">'Xtest'</span>), mat_test.get(<span class="string">'ytest'</span>).ravel()</span><br><span class="line"></span><br><span class="line">svc = svm.SVC()</span><br><span class="line">svc.fit(X, y)</span><br><span class="line"></span><br><span class="line">pred = svc.predict(test_X)</span><br><span class="line">print(metrics.classification_report(test_y, pred))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 逻辑回归试试</span></span><br><span class="line">logit = LogisticRegression()</span><br><span class="line">logit.fit(X, y)</span><br><span class="line">pred = logit.predict(test_X)</span><br><span class="line">print(metrics.classification_report(test_y, pred))</span><br></pre></td></tr></table></figure>
<h1 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h1><h2 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h2><h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><p>随机选择K个聚类中心</p>
<p>重复{</p>
<ol>
<li>将所有样本点划分在这K类中</li>
<li>在每个类中重新计算聚类中心</li>
</ol>
<p>}直至聚类中心不再变化</p>
<h3 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h3><p>符号：</p>
<p>$c^{(i)}$—样本$x^{(i)}$当前所属的类</p>
<p>$\mu_k$—第k个类别的中心</p>
<p>$\mu_c^{(i)}$—样本x(i)所属类的聚类中心</p>
<p>目标：$\underbrace \min _{c^{( 1 )},…,c^{( m )},\mu _1,…,\mu _K}J( c^{( 1 )},…,c^{( m )},\mu _1,…,\mu _K ) = \frac{1}{m}\sum\limits_{i = 1}^m \parallel x^{(i)}-\mu_c^{(i)} \parallel^2$</p>
<h3 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h3><p>通常多次运行K-means算法，选取代价函数最小的结果。当K=(2-10)时可行。若K值较大，这么做也可能不会有明显改善</p>
<h3 id="K值选择"><a href="#K值选择" class="headerlink" title="K值选择"></a>K值选择</h3><ul>
<li>“肘部法则”</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/006M8Ovlly1g9ii4rukprj309f060q3a.jpg" alt="肘部法则.png"></p>
<ul>
<li>根据聚类后的“后续目的”人工选择</li>
</ul>

      
    </div>
    
    
    

    
      <div>
        
<div style="text-align:center;color: #ccc;font-size:14px;">
------ 本文结束<i class="fa fa-paw"></i>感谢您的阅读 ------</div>

      </div>
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/11/15/Tensorflow2.0与深度学习/" rel="next" title="Tensorflow2.0">
                <i class="fa fa-chevron-left"></i> Tensorflow2.0
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/12/26/What Are You Known For Learning User Topical Profiles with Implicit and Explicit Footprints/" rel="prev" title="What Are You Known For? Learning User Topical Profiles with Implicit and Explicit Footprints">
                What Are You Known For? Learning User Topical Profiles with Implicit and Explicit Footprints <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/westbrook.jpg" alt="Kason">
            
              <p class="site-author-name" itemprop="name">Kason</p>
              <p class="site-description motion-element" itemprop="description">Kason's technology blog</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Kasonreal" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:shiyuxiang99@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://tristone13th.github.io/" title="云中君" target="_blank">云中君</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://101.201.69.42/" title="木偶" target="_blank">木偶</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#机器学习引言"><span class="nav-number">1.</span> <span class="nav-text">机器学习引言</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#监督学习-Supervised-learning"><span class="nav-number">1.1.</span> <span class="nav-text">监督学习(Supervised learning)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#监督学习的类型"><span class="nav-number">1.1.1.</span> <span class="nav-text">监督学习的类型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#非监督学习-Unsupervised-learning"><span class="nav-number">1.2.</span> <span class="nav-text">非监督学习(Unsupervised learning)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#非监督学习的类型"><span class="nav-number">1.2.1.</span> <span class="nav-text">非监督学习的类型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#聚类的应用："><span class="nav-number">1.2.2.</span> <span class="nav-text">聚类的应用：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#线性回归-Linear-Regression"><span class="nav-number">2.</span> <span class="nav-text">线性回归(Linear Regression)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#描述回归问题的标记"><span class="nav-number">2.1.</span> <span class="nav-text">描述回归问题的标记</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度下降实践"><span class="nav-number">2.2.</span> <span class="nav-text">梯度下降实践</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正规方程"><span class="nav-number">2.3.</span> <span class="nav-text">正规方程</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#逻辑回归"><span class="nav-number">3.</span> <span class="nav-text">逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#二分类"><span class="nav-number">3.1.</span> <span class="nav-text">二分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#一对多"><span class="nav-number">3.2.</span> <span class="nav-text">一对多</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Exercise-多项式特征映射"><span class="nav-number">3.3.</span> <span class="nav-text">Exercise(多项式特征映射)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Question"><span class="nav-number">3.4.</span> <span class="nav-text">Question</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#正则化"><span class="nav-number">4.</span> <span class="nav-text">正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化线性回归"><span class="nav-number">4.1.</span> <span class="nav-text">正则化线性回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化逻辑回归"><span class="nav-number">4.2.</span> <span class="nav-text">正则化逻辑回归</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络"><span class="nav-number">5.</span> <span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#向前传播"><span class="nav-number">5.1.</span> <span class="nav-text">向前传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播"><span class="nav-number">5.2.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Exercise（手写数字识别）"><span class="nav-number">5.3.</span> <span class="nav-text">Exercise（手写数字识别）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#正向传播"><span class="nav-number">5.3.1.</span> <span class="nav-text">正向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播-1"><span class="nav-number">5.3.2.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度校验"><span class="nav-number">5.3.3.</span> <span class="nav-text">梯度校验</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#机器学习诊断法"><span class="nav-number">6.</span> <span class="nav-text">机器学习诊断法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#模型选择"><span class="nav-number">6.1.</span> <span class="nav-text">模型选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#偏差-Bias-和方差-Variance"><span class="nav-number">6.2.</span> <span class="nav-text">偏差(Bias)和方差(Variance)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化-1"><span class="nav-number">6.3.</span> <span class="nav-text">正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#学习曲线"><span class="nav-number">6.4.</span> <span class="nav-text">学习曲线</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#小结"><span class="nav-number">6.5.</span> <span class="nav-text">小结</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络中的偏差和方差"><span class="nav-number">6.5.1.</span> <span class="nav-text">神经网络中的偏差和方差</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#机器学习系统设计"><span class="nav-number">7.</span> <span class="nav-text">机器学习系统设计</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#学习算法的推荐方法"><span class="nav-number">7.1.</span> <span class="nav-text">学习算法的推荐方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#类偏斜"><span class="nav-number">7.2.</span> <span class="nav-text">类偏斜</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#机器学习数据"><span class="nav-number">7.3.</span> <span class="nav-text">机器学习数据</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#支持向量机（SVM"><span class="nav-number">8.</span> <span class="nav-text">支持向量机（SVM)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#假设函数"><span class="nav-number">8.1.</span> <span class="nav-text">假设函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#大间距分类器（Large-Margin-Classifier"><span class="nav-number">8.2.</span> <span class="nav-text">大间距分类器（Large Margin Classifier)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#内核"><span class="nav-number">8.3.</span> <span class="nav-text">内核</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SVM使用"><span class="nav-number">8.4.</span> <span class="nav-text">SVM使用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Exercise"><span class="nav-number">8.5.</span> <span class="nav-text">Exercise</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#线性内核"><span class="nav-number">8.5.1.</span> <span class="nav-text">线性内核</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gaussion-kernels"><span class="nav-number">8.5.2.</span> <span class="nav-text">Gaussion kernels</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#选择参数"><span class="nav-number">8.5.3.</span> <span class="nav-text">选择参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#垃圾邮件分类"><span class="nav-number">8.5.4.</span> <span class="nav-text">垃圾邮件分类</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#无监督学习"><span class="nav-number">9.</span> <span class="nav-text">无监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#K-means"><span class="nav-number">9.1.</span> <span class="nav-text">K-means</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#算法步骤"><span class="nav-number">9.1.1.</span> <span class="nav-text">算法步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#优化目标"><span class="nav-number">9.1.2.</span> <span class="nav-text">优化目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机初始化"><span class="nav-number">9.1.3.</span> <span class="nav-text">随机初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K值选择"><span class="nav-number">9.1.4.</span> <span class="nav-text">K值选择</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kason</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'R4rICLTQsNXFhLpVLOKUtwY8-MdYXbMMI',
        appKey: 'a3s2kYOiN3ocHpSSUvnOw2BM',
        placeholder: '来说两句吧',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  
  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
